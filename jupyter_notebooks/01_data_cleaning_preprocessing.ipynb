{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Cleaning & Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* This notebook comprises the data cleaning and preprocessing steps necessary to ensure the data is in a suitable format for subsequent visualisations and analysis.\n",
        "\n",
        "## Inputs\n",
        "* Raw Dataset: `01_RAW_NEA-Seafloor-Litter.csv`. \n",
        "\n",
        "    This served as the initial data source.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Processed Dataset: `02_PROCESSED_NEA-Seafloor-Litter.csv`. \n",
        "\n",
        "    This is the output file of the data cleaning and preprocessing operations.\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* The preprocessing phase was conducted to ensure data integrity, address inconsistencies, and prepare the dataset for rigorous analysis. This included the removal of missing data, the standardisation of data types, and the elimination of extraneous variables, thereby enhancing the dataset's reliability and analytical utility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/mahahussain/Desktop/NEA-Seafloor-Litter-Analysis/NEA-Seafloor-Litter-Analysis/jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/mahahussain/Desktop/NEA-Seafloor-Litter-Analysis/NEA-Seafloor-Litter-Analysis'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Extract: Importing Libraries & Extracting Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section involves:\n",
        "- Importing the required libraries for data manipulation and subsequent operations.\n",
        "- Loading the data from a CSV file into a Pandas DataFrame as the Pandas library contains helpful methods for loading, cleaning, and transforming data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "'''\n",
        "Importing necessary libraries\n",
        "    1) Pandas: for data manipulation and analysis\n",
        "    2) NumPy: for numerical operations\n",
        "'''\n",
        "import pandas as pd \n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After importing these libraries, load in the raw dataset using the `read_csv()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset in with read_csv()\n",
        "raw_df = pd.read_csv(\"data/01_RAW_NEA-Seafloor-Litter.csv\")\n",
        "\n",
        "# Verify successful operation\n",
        "print(\"Dataset loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1.1 Evaluation:**\n",
        "\n",
        "I chose to read the dataset using pd.read_csv() because it's a standard and efficient method for importing CSV files into a DataFrame in Python. \n",
        "\n",
        "I used the read_csv() function to load the raw data and verify its successful operation by printing a confirmation message. \n",
        "\n",
        "If I had more time, I could have explored different file formats or added parameters to handle specific edge cases (e.g., encoding issues or large file chunks). \n",
        "\n",
        "A limitation of this method is that it assumes the CSV file is correctly formatted, and it may not handle errors or inconsistencies in the data without additional checks or cleaning steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset is correctly loaded and ready for further processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Understanding the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section involves:\n",
        "- Examining dataset structure and datatypes.\n",
        "- Correcting incorrect data types.\n",
        "- Identifying missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2.1 Data Type Inspection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**In this subsection:**\n",
        "\n",
        "A function is created to display the following:\n",
        "- `data_frame.shape`  returns the dimensionality of the DataFrame.\n",
        "- `info()` method to print data types and non-null counts for each column.\n",
        "- `head()` method to display the first five rows for inspection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ----- Total Rows and Columns -----\n",
            "(4310, 62)\n",
            "\n",
            "\n",
            " ----- Data Types -----\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4310 entries, 0 to 4309\n",
            "Data columns (total 62 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   year               4310 non-null   int64  \n",
            " 1   survey             4310 non-null   object \n",
            " 2   cruise             4310 non-null   object \n",
            " 3   area               4033 non-null   object \n",
            " 4   station            4310 non-null   int64  \n",
            " 5   fldHaulLatDegrees  4310 non-null   int64  \n",
            " 6   fldHaulLatMinutes  4052 non-null   float64\n",
            " 7   fldHaulLonDegrees  4310 non-null   int64  \n",
            " 8   fldHaulLonMinutes  4052 non-null   float64\n",
            " 9   fldHaulEorW        4057 non-null   object \n",
            " 10  fldShotLatDegrees  3829 non-null   float64\n",
            " 11  fldShotLatMinutes  3603 non-null   float64\n",
            " 12  fldShotLonDegrees  3745 non-null   float64\n",
            " 13  fldShotLonMinutes  3519 non-null   float64\n",
            " 14  fldShotEorW        3519 non-null   object \n",
            " 15  Latitude           4310 non-null   float64\n",
            " 16  Longitude          4310 non-null   float64\n",
            " 17  fldDateTimeShot    4310 non-null   object \n",
            " 18  bottle             4310 non-null   int64  \n",
            " 19  sheet              4310 non-null   int64  \n",
            " 20  bag                4310 non-null   int64  \n",
            " 21  caps               4310 non-null   int64  \n",
            " 22  fishline.mono      4310 non-null   int64  \n",
            " 23  fishline.tang      4310 non-null   int64  \n",
            " 24  synthrope          4310 non-null   int64  \n",
            " 25  fishnet            4310 non-null   int64  \n",
            " 26  cabletie           4310 non-null   int64  \n",
            " 27  strap              4310 non-null   int64  \n",
            " 28  crates             4310 non-null   int64  \n",
            " 29  nappies            4310 non-null   int64  \n",
            " 30  santowels          4310 non-null   int64  \n",
            " 31  other.plas         4310 non-null   int64  \n",
            " 32  cansfood           4310 non-null   int64  \n",
            " 33  cansdrink          4310 non-null   int64  \n",
            " 34  fishmetal          4310 non-null   int64  \n",
            " 35  drums              4310 non-null   int64  \n",
            " 36  appliance          4310 non-null   int64  \n",
            " 37  carparts           4310 non-null   int64  \n",
            " 38  cables             4310 non-null   int64  \n",
            " 39  other.metal        4310 non-null   int64  \n",
            " 40  wellies            4310 non-null   int64  \n",
            " 41  balloon            4310 non-null   int64  \n",
            " 42  bobbins            4310 non-null   int64  \n",
            " 43  tyre               4310 non-null   int64  \n",
            " 44  gloves             4310 non-null   int64  \n",
            " 45  other.rub          4310 non-null   int64  \n",
            " 46  jars               4310 non-null   int64  \n",
            " 47  bottles            4310 non-null   int64  \n",
            " 48  pieces             4310 non-null   int64  \n",
            " 49  other.glass        4310 non-null   int64  \n",
            " 50  woodnat            4310 non-null   int64  \n",
            " 51  woodproc           4310 non-null   int64  \n",
            " 52  rope               4310 non-null   int64  \n",
            " 53  paper              4310 non-null   int64  \n",
            " 54  pallets            4310 non-null   int64  \n",
            " 55  other.wood         4310 non-null   int64  \n",
            " 56  clothing           4310 non-null   int64  \n",
            " 57  shoes              4310 non-null   int64  \n",
            " 58  other.misc         4310 non-null   int64  \n",
            " 59  totallitter        4310 non-null   int64  \n",
            " 60  distance           4310 non-null   int64  \n",
            " 61  wingspread         4310 non-null   int64  \n",
            "dtypes: float64(8), int64(48), object(6)\n",
            "memory usage: 2.0+ MB\n",
            "None\n",
            "\n",
            "\n",
            " ----- First Five Rows -----\n",
            "   year survey     cruise               area  station  fldHaulLatDegrees  \\\n",
            "0  1992   IBTS  CIRO 9/92  Greater North Sea        1                 51   \n",
            "1  1992   IBTS  CIRO 9/92  Greater North Sea        2                 51   \n",
            "2  1992   IBTS  CIRO 9/92  Greater North Sea        3                 51   \n",
            "3  1992   IBTS  CIRO 9/92  Greater North Sea        4                 52   \n",
            "4  1992   IBTS  CIRO 9/92  Greater North Sea        5                 52   \n",
            "\n",
            "   fldHaulLatMinutes  fldHaulLonDegrees  fldHaulLonMinutes fldHaulEorW  ...  \\\n",
            "0               44.3                  1               45.2           E  ...   \n",
            "1               36.1                  2               47.8           E  ...   \n",
            "2               49.4                  3               38.6           E  ...   \n",
            "3               49.4                  2               45.6           E  ...   \n",
            "4               41.1                  3               24.7           E  ...   \n",
            "\n",
            "   rope  paper  pallets  other.wood clothing  shoes  other.misc totallitter  \\\n",
            "0     0      0        0           0        0      0           0           1   \n",
            "1     0      0        0           0        0      0           0           2   \n",
            "2     0      0        0           0        0      0           0           1   \n",
            "3     0      0        0           0        0      0           0           1   \n",
            "4     0      0        0           0        0      0           0           2   \n",
            "\n",
            "   distance  wingspread  \n",
            "0      3794           0  \n",
            "1      3918           0  \n",
            "2      3624           0  \n",
            "3      3642           0  \n",
            "4      3791           0  \n",
            "\n",
            "[5 rows x 62 columns]\n"
          ]
        }
      ],
      "source": [
        "def check_data_and_types(data_frame):\n",
        "    \"\"\"\n",
        "    This function shows the type of data for each column and the first 5 rows of the dataset.\n",
        "\n",
        "    Args: data_frame: The data frame we want to inspect.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\" ----- Total Rows and Columns -----\")\n",
        "    print(data_frame.shape)\n",
        "    print ('\\n')\n",
        "\n",
        "    print(\" ----- Data Types -----\")\n",
        "    print(data_frame.info())\n",
        "    print ('\\n')\n",
        "    \n",
        "    print(\" ----- First Five Rows -----\")\n",
        "    print(data_frame.head())\n",
        "\n",
        "    \n",
        "\n",
        "check_data_and_types(raw_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**From this, we observe:**\n",
        "\n",
        "- Numerical Columns: Columns like year, station, fldHaulLatDegrees, fldHaulLonDegrees, Latitude, Longitude, etc., are of numerical data types like int64 and float64.\n",
        "\n",
        "- Categorical Columns: Columns like survey, cruise, area, and fldHaulEorW are of object type, which means they are categorical columns.\n",
        "\n",
        "- Date/Time Columns: The fldDateTimeShot column is of object type, which might need conversion into a proper datetime format for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What Needs to Be Changed?**\n",
        "\n",
        "- Date Column (fldDateTimeShot): This column should be converted to a proper datetime type, as it is currently stored as an object (string). \n",
        "\n",
        "This will allow for easier manipulation and time-based analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2.2 Date Format Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversion successful!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "dtype('<M8[ns]')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the 'fldDateTimeShot' column to datetime\n",
        "raw_df['fldDateTimeShot'] = pd.to_datetime(raw_df['fldDateTimeShot'], errors = 'coerce', dayfirst=True)\n",
        "\n",
        "# errors = 'coerce' will convert any errors to blank values\n",
        "# dayfirst = True will convert the date to the format 'dd/mm/yyyy'\n",
        "\n",
        "print(\"Conversion successful!\")\n",
        "\n",
        "# Check the data types of the date column\n",
        "raw_df['fldDateTimeShot'].dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`dtype('<M8[ns]') ` is a datetime format - conversion successful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2.3 Missing Value Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section:\n",
        "- Use isna() to identify missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "area                 277\n",
            "fldHaulLatMinutes    258\n",
            "fldHaulLonMinutes    258\n",
            "fldHaulEorW          253\n",
            "fldShotLatDegrees    481\n",
            "fldShotLatMinutes    707\n",
            "fldShotLonDegrees    565\n",
            "fldShotLonMinutes    791\n",
            "fldShotEorW          791\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def get_missing_values(data_frame):\n",
        "    \"\"\"\n",
        "    This function returns the number of missing values in the dataset.\n",
        "\n",
        "    Args: data_frame: The data frame we want to inspect.\n",
        "    \"\"\"\n",
        "    missing_values = data_frame.isna().sum() # Get the number of missing values in each column\n",
        "    print(missing_values[missing_values > 0]) # Only print columns with missing values\n",
        "\n",
        "get_missing_values(raw_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can utilise this list to target missing values in the handling section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Handling Missing Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This sectio covers: #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3.1 Missing Area Data Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ToDO:\n",
        "- cross check, check area col w other cols, e.g., if all station x/survey x is sea A then we can impute it, else remove.\n",
        "- if a row does not contain "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AI tool: Github Copilot was used to generate a cross-checking function for area, it was then improved by ChatGPT to impute the rows with 1 asoc sea with the station."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial missing values: 277\n",
            "Station 2 has multiple or no sea associations.\n",
            "Station 3 has multiple or no sea associations.\n",
            "Station 5 has multiple or no sea associations.\n",
            "Station 6 has multiple or no sea associations.\n",
            "Station 7 has multiple or no sea associations.\n",
            "Station 8 has multiple or no sea associations.\n",
            "Station 10 has multiple or no sea associations.\n",
            "Station 11 has multiple or no sea associations.\n",
            "Station 12 has multiple or no sea associations.\n",
            "Station 14 has multiple or no sea associations.\n",
            "Station 15 has multiple or no sea associations.\n",
            "Station 16 has multiple or no sea associations.\n",
            "Station 17 has multiple or no sea associations.\n",
            "Station 18 has multiple or no sea associations.\n",
            "Station 19 has multiple or no sea associations.\n",
            "Station 21 has multiple or no sea associations.\n",
            "Station 22 has multiple or no sea associations.\n",
            "Station 23 has multiple or no sea associations.\n",
            "Station 24 has multiple or no sea associations.\n",
            "Station 26 has multiple or no sea associations.\n",
            "Station 27 has multiple or no sea associations.\n",
            "Station 28 has multiple or no sea associations.\n",
            "Station 29 has multiple or no sea associations.\n",
            "Station 30 has multiple or no sea associations.\n",
            "Station 33 has multiple or no sea associations.\n",
            "Station 34 has multiple or no sea associations.\n",
            "Station 35 has multiple or no sea associations.\n",
            "Station 37 has multiple or no sea associations.\n",
            "Station 39 has multiple or no sea associations.\n",
            "Station 40 has multiple or no sea associations.\n",
            "Station 42 has multiple or no sea associations.\n",
            "Station 44 has multiple or no sea associations.\n",
            "Station 45 has multiple or no sea associations.\n",
            "Station 46 has multiple or no sea associations.\n",
            "Station 47 has multiple or no sea associations.\n",
            "Station 48 has multiple or no sea associations.\n",
            "Station 50 has multiple or no sea associations.\n",
            "Station 51 has multiple or no sea associations.\n",
            "Station 52 has multiple or no sea associations.\n",
            "Station 53 has multiple or no sea associations.\n",
            "Station 55 has multiple or no sea associations.\n",
            "Station 56 has multiple or no sea associations.\n",
            "Station 57 has multiple or no sea associations.\n",
            "Station 58 has multiple or no sea associations.\n",
            "Station 59 has multiple or no sea associations.\n",
            "Station 61 has multiple or no sea associations.\n",
            "Station 62 has multiple or no sea associations.\n",
            "Station 64 has multiple or no sea associations.\n",
            "Station 65 has multiple or no sea associations.\n",
            "Station 66 has multiple or no sea associations.\n",
            "Station 68 has multiple or no sea associations.\n",
            "Station 69 has multiple or no sea associations.\n",
            "Station 70 has multiple or no sea associations.\n",
            "Station 72 has multiple or no sea associations.\n",
            "Station 73 has multiple or no sea associations.\n",
            "Station 74 has multiple or no sea associations.\n",
            "Station 76 has multiple or no sea associations.\n",
            "Station 77 has multiple or no sea associations.\n",
            "Station 78 has multiple or no sea associations.\n",
            "Station 79 has multiple or no sea associations.\n",
            "Imputed Celtic Seas for Station 81.\n",
            "Station 82 has multiple or no sea associations.\n",
            "Station 83 has multiple or no sea associations.\n",
            "Imputed Celtic Seas for Station 84.\n",
            "Imputed Celtic Seas for Station 87.\n",
            "Imputed Celtic Seas for Station 88.\n",
            "Imputed Celtic Seas for Station 89.\n",
            "Imputed Celtic Seas for Station 91.\n",
            "Imputed Celtic Seas for Station 92.\n",
            "Station 93 has multiple or no sea associations.\n",
            "Station 95 has multiple or no sea associations.\n",
            "Station 97 has multiple or no sea associations.\n",
            "Station 98 has multiple or no sea associations.\n",
            "Station 100 has multiple or no sea associations.\n",
            "Station 102 has multiple or no sea associations.\n",
            "Station 103 has multiple or no sea associations.\n",
            "Station 104 has multiple or no sea associations.\n",
            "Imputed Celtic Seas for Station 106.\n",
            "Imputed Celtic Seas for Station 107.\n",
            "Imputed Celtic Seas for Station 110.\n",
            "Imputed Celtic Seas for Station 111.\n",
            "Imputed Celtic Seas for Station 112.\n",
            "Imputed Celtic Seas for Station 113.\n",
            "Imputed Celtic Seas for Station 115.\n",
            "Imputed Celtic Seas for Station 116.\n",
            "Imputed Celtic Seas for Station 118.\n",
            "Station 119 has multiple or no sea associations.\n",
            "Imputed Celtic Seas for Station 120.\n",
            "Imputed Celtic Seas for Station 121.\n",
            "Station 123 has multiple or no sea associations.\n",
            "Imputed Celtic Seas for Station 124.\n",
            "Imputed Celtic Seas for Station 125.\n",
            "Imputed Celtic Seas for Station 126.\n",
            "Imputed Celtic Seas for Station 128.\n",
            "Imputed Celtic Seas for Station 129.\n",
            "Imputed Celtic Seas for Station 131.\n",
            "Imputed Celtic Seas for Station 133.\n",
            "Imputed Celtic Seas for Station 134.\n",
            "Imputed Celtic Seas for Station 136.\n",
            "Imputed Celtic Seas for Station 137.\n",
            "Imputed Celtic Seas for Station 139.\n",
            "Imputed Celtic Seas for Station 140.\n",
            "Imputed Celtic Seas for Station 141.\n",
            "Imputed Celtic Seas for Station 144.\n",
            "Imputed Celtic Seas for Station 145.\n",
            "Imputed Celtic Seas for Station 147.\n",
            "Imputed Celtic Seas for Station 148.\n",
            "Station 149 has multiple or no sea associations.\n",
            "Imputed Celtic Seas for Station 151.\n",
            "Imputed Celtic Seas for Station 152.\n",
            "Imputed Celtic Seas for Station 154.\n",
            "Station 155 has multiple or no sea associations.\n",
            "Imputed Celtic Seas for Station 156.\n",
            "Station 157 has multiple or no sea associations.\n",
            "Imputed Celtic Seas for Station 158.\n",
            "Imputed Celtic Seas for Station 160.\n",
            "Imputed Celtic Seas for Station 161.\n",
            "Imputed Celtic Seas for Station 162.\n",
            "Imputed Celtic Seas for Station 163.\n",
            "Imputed Celtic Seas for Station 164.\n",
            "Station 166 has multiple or no sea associations.\n",
            "Imputed Celtic Seas for Station 167.\n",
            "Imputed Celtic Seas for Station 168.\n",
            "Imputed Celtic Seas for Station 169.\n",
            "Imputed Celtic Seas for Station 170.\n",
            "Imputed Celtic Seas for Station 172.\n",
            "Imputed Celtic Seas for Station 173.\n",
            "Imputed Celtic Seas for Station 174.\n",
            "Imputed Celtic Seas for Station 175.\n",
            "Imputed Celtic Seas for Station 176.\n",
            "Imputed Celtic Seas for Station 177.\n",
            "Imputed Celtic Seas for Station 179.\n",
            "Imputed Celtic Seas for Station 180.\n",
            "Imputed Celtic Seas for Station 181.\n",
            "Imputed Celtic Seas for Station 182.\n",
            "Imputed Celtic Seas for Station 184.\n",
            "Imputed Celtic Seas for Station 185.\n",
            "Imputed Celtic Seas for Station 186.\n",
            "Station 96 has multiple or no sea associations.\n",
            "Missing values after imputation: 153\n",
            "area                 153\n",
            "fldHaulLatMinutes    258\n",
            "fldHaulLonMinutes    258\n",
            "fldHaulEorW          253\n",
            "fldShotLatDegrees    481\n",
            "fldShotLatMinutes    707\n",
            "fldShotLonDegrees    565\n",
            "fldShotLonMinutes    791\n",
            "fldShotEorW          791\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def find_sea_by_station_and_impute(data_frame):\n",
        "    \"\"\"\n",
        "    Imputes missing 'area' values based on station associations.\n",
        "    Returns the modified DataFrame explicitly.\n",
        "    \"\"\"\n",
        "    missing_area_rows = data_frame[data_frame['area'].isna()]  \n",
        "    print(f\"Initial missing values: {missing_area_rows.shape[0]}\")  \n",
        "\n",
        "    missing_areas_stations = missing_area_rows['station'].unique()  \n",
        "\n",
        "    for station in missing_areas_stations:\n",
        "        asoc_seas = data_frame[data_frame['station'] == station]['area'].dropna().unique()  \n",
        "\n",
        "        if len(asoc_seas) == 1:\n",
        "            sea_to_impute = asoc_seas[0]\n",
        "            data_frame.loc[(data_frame['station'] == station) & (data_frame['area'].isna()), 'area'] = sea_to_impute\n",
        "            print(f\"Imputed {sea_to_impute} for Station {station}.\")\n",
        "        else:\n",
        "            print(f\"Station {station} has multiple or no sea associations.\")\n",
        "\n",
        "    print(f\"Missing values after imputation: {data_frame['area'].isna().sum()}\")  \n",
        "    return data_frame  \n",
        "\n",
        "# Call function and reassign raw_df\n",
        "raw_df = find_sea_by_station_and_impute(raw_df)\n",
        "\n",
        "# Verify successful operation\n",
        "get_missing_values(raw_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "area                 153\n",
            "fldHaulLatMinutes    258\n",
            "fldHaulLonMinutes    258\n",
            "fldHaulEorW          253\n",
            "fldShotLatDegrees    481\n",
            "fldShotLatMinutes    707\n",
            "fldShotLonDegrees    565\n",
            "fldShotLonMinutes    791\n",
            "fldShotEorW          791\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "get_missing_values(raw_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3.2 Removal of Ambiguous Sea Area Entries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#TODO Now we can fill the remaining missing area's with \"Unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing area values after imputation: 0\n",
            "\n",
            "fldHaulLatMinutes    258\n",
            "fldHaulLonMinutes    258\n",
            "fldHaulEorW          253\n",
            "fldShotLatDegrees    481\n",
            "fldShotLatMinutes    707\n",
            "fldShotLonDegrees    565\n",
            "fldShotLonMinutes    791\n",
            "fldShotEorW          791\n",
            "dtype: int64\n",
            "\n",
            " Missing area values filled successfully!\n"
          ]
        }
      ],
      "source": [
        "#Fill NaN values in 'area' with \"Unknown\"\n",
        "raw_df['area'].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "# Verify successful operation\n",
        "print(f\"Missing area values after imputation: {raw_df['area'].isna().sum()}\\n\")\n",
        "\n",
        "# Verify successful operation\n",
        "get_missing_values(raw_df)\n",
        "\n",
        "print(\"\\n Missing area values filled successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that `area` has been imputed successfully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3.3 Remvoing Unnecessary Location Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `fld ...` columns exhibit a significant proportion of missing values. Given that one of the primary objectives of this project is to visualise the data, these columns will simply be removed with the exception of the date. \n",
        "\n",
        "Note that the `Latitude` and `Longitude` columns will be retained, as they are complete within the dataset, and are sufficient variables for spatial visualisations.\n",
        "\n",
        "Removal of these columns as opposed to row deletion will minimise the data loss and streamline the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Series([], dtype: int64)\n"
          ]
        }
      ],
      "source": [
        "# Remove the fld columns \n",
        "cols_to_remove = [\n",
        "    'fldHaulLatMinutes',\n",
        "    'fldHaulLonMinutes',\n",
        "    'fldHaulEorW',\n",
        "    'fldShotLatDegrees',\n",
        "    'fldShotLatMinutes',\n",
        "    'fldHaulLatDegrees',\n",
        "    'fldHaulLonDegrees',\n",
        "    'fldShotLonDegrees',\n",
        "    'fldShotLonMinutes',\n",
        "    'fldShotEorW',\n",
        "]\n",
        "\n",
        "raw_df = raw_df.drop(columns=cols_to_remove, axis=1)\n",
        "\n",
        "get_missing_values(raw_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3.4 Renaming DateTime column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rename fldDateTimeShot to 'datetime'\n",
        "raw_df.rename(columns={'fldDateTimeShot': 'date'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Removing Duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4.1 Duplicate Row Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 0 duplicate rows in the dataset.\n"
          ]
        }
      ],
      "source": [
        "# Remove duplicate rows\n",
        "raw_df.drop_duplicates(inplace=True)\n",
        "\n",
        "print(f\"There are {raw_df.duplicated().sum()} duplicate rows in the dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 Category Mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to faciliate the analysis of the marine litter composition, the items must first be categorised.\n",
        "\n",
        "Utilising the information contained within the CEFAS data code book, the items were classified based on their primary materials.\n",
        "\n",
        "The categories established were as follows:\n",
        "\n",
        "- `Plastic:` This category includes items composed of synthetic polymers, encompassing bottles, sheets, bags, and various fishing-related plastics.\n",
        "\n",
        "- `Metal:` This category includes items composed of metallic substances, such as cans, drums, and metal components from appliances.\n",
        "\n",
        "- `Rubber:` Items made from natural or synthetic rubber, including boots, balloons, and tires, were classified within this category.\n",
        "\n",
        "- `Glass:` Items composed of silica-based materials, such as jars, bottles, and glass fragments, were grouped within this category.\n",
        "\n",
        "- ` Plant-Based Materials:` This category includes items derived from plant matter, encompassing both natural and processed wood, paper, and plant-based rope.\n",
        "\n",
        "- `Clothing & Fabrics:` Textiles and related items, such as clothing and shoes, were classified within this category.\n",
        "\n",
        "- `Miscellaneous:` This category served as a catch-all for items that did not readily fit into the aforementioned categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "categories = {\n",
        "    'Plastic': ['bottle', 'sheet', 'bag', 'caps', 'fishline.mono',\n",
        "        'fishline.tang', 'synthrope', 'fishnet', 'cabletie',\n",
        "        'strap', 'nappies', 'crates', 'santowels', 'other.plas'],\n",
        "\n",
        "    'Metal': ['cansfood', 'cansdrink', 'fishmetal', 'drums',\n",
        "        'appliance', 'carparts', 'cables', 'other.metal'],\n",
        "\n",
        "    'Rubber': ['wellies', 'balloon', 'bobbins', 'tyre', 'gloves', 'other.rub'],\n",
        "\n",
        "    'Glass': ['jars', 'bottles', 'pieces', 'other.glass'],\n",
        "\n",
        "    'Plant-Based Materials': ['woodnat', 'woodproc', 'rope', 'paper', 'pallets', 'other.wood'],\n",
        "\n",
        "    'Clothing & Fabrics': ['clothing', 'shoes'],\n",
        "\n",
        "    'Miscellaneous': ['other.misc'],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  1.6 Checking Numerical Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.6.1 Confirming Numerical Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['year', 'station', 'Latitude', 'Longitude', 'date', 'bottle', 'sheet', 'bag', 'caps', 'fishline.mono', 'fishline.tang', 'synthrope', 'fishnet', 'cabletie', 'strap', 'crates', 'nappies', 'santowels', 'other.plas', 'cansfood', 'cansdrink', 'fishmetal', 'drums', 'appliance', 'carparts', 'cables', 'other.metal', 'wellies', 'balloon', 'bobbins', 'tyre', 'gloves', 'other.rub', 'jars', 'bottles', 'pieces', 'other.glass', 'woodnat', 'woodproc', 'rope', 'paper', 'pallets', 'other.wood', 'clothing', 'shoes', 'other.misc', 'totallitter', 'distance', 'wingspread']\n"
          ]
        }
      ],
      "source": [
        "# Numerical Columns:\n",
        "numerical_cols = raw_df.dtypes[raw_df.dtypes != 'object'].index.tolist()\n",
        "print(numerical_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.6.2 Verifying Latitude and Longitude "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Latitudes should be between -90 and 90.\n",
        "Longitudes should be between -180 and 180."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the minumum latitude is:  48.045\n",
            "the maximum latitude is:  61.57\n",
            "the minumum longitude is:  -9.138666667\n",
            "the maximum longitude is:  7.5355\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Latitude and Longitude Columns:\n",
        "print(\"the minumum latitude is: \", raw_df['Latitude'].min())\n",
        "print(\"the maximum latitude is: \", raw_df['Latitude'].max())\n",
        "\n",
        "print(\"the minumum longitude is: \", raw_df['Longitude'].min())\n",
        "print(\"the maximum longitude is: \", raw_df['Longitude'].max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.7 Verifying Final Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.7.1 Checking for Missing Values - Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Missing Values ---\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "missing = raw_df.isna().sum()\n",
        "\n",
        "print(\"--- Missing Values ---\")\n",
        "print(missing[missing > 0].sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.7.2 CheckingDataFrame shape - Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " --- Shape of the Dataset ---\n",
            "(4307, 52)\n"
          ]
        }
      ],
      "source": [
        "# View shape of the dataset\n",
        "print(\"\\n --- Shape of the Dataset ---\")\n",
        "print(raw_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.7.3 Checking DataFrame Head - Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " --- First 5 Rows of the Dataset ---\n",
            "   year survey     cruise               area  station   Latitude  Longitude  \\\n",
            "0  1992   IBTS  CIRO 9/92  Greater North Sea        1  51.738333   1.753333   \n",
            "1  1992   IBTS  CIRO 9/92  Greater North Sea        2  51.601667   2.796667   \n",
            "2  1992   IBTS  CIRO 9/92  Greater North Sea        3  51.823333   3.643333   \n",
            "3  1992   IBTS  CIRO 9/92  Greater North Sea        4  52.823333   2.760000   \n",
            "4  1992   IBTS  CIRO 9/92  Greater North Sea        5  52.685000   3.411667   \n",
            "\n",
            "        date  bottle  sheet  ...  rope  paper  pallets  other.wood  clothing  \\\n",
            "0 1992-08-14       0      0  ...     0      0        0           0         0   \n",
            "1 1992-08-15       0      0  ...     0      0        0           0         0   \n",
            "2 1992-08-15       0      0  ...     0      0        0           0         0   \n",
            "3 1992-08-16       0      0  ...     0      0        0           0         0   \n",
            "4 1992-08-16       1      0  ...     0      0        0           0         0   \n",
            "\n",
            "   shoes  other.misc  totallitter  distance  wingspread  \n",
            "0      0           0            1      3794           0  \n",
            "1      0           0            2      3918           0  \n",
            "2      0           0            1      3624           0  \n",
            "3      0           0            1      3642           0  \n",
            "4      0           0            2      3791           0  \n",
            "\n",
            "[5 rows x 52 columns]\n"
          ]
        }
      ],
      "source": [
        "# First 5 rows of the dataset\n",
        "print(\"\\n --- First 5 Rows of the Dataset ---\")\n",
        "print(raw_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.7.4 Checking DataTypes - Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " --- Data Types ---\n",
            "year                      int64\n",
            "survey                   object\n",
            "cruise                   object\n",
            "area                     object\n",
            "station                   int64\n",
            "Latitude                float64\n",
            "Longitude               float64\n",
            "date             datetime64[ns]\n",
            "bottle                    int64\n",
            "sheet                     int64\n",
            "bag                       int64\n",
            "caps                      int64\n",
            "fishline.mono             int64\n",
            "fishline.tang             int64\n",
            "synthrope                 int64\n",
            "fishnet                   int64\n",
            "cabletie                  int64\n",
            "strap                     int64\n",
            "crates                    int64\n",
            "nappies                   int64\n",
            "santowels                 int64\n",
            "other.plas                int64\n",
            "cansfood                  int64\n",
            "cansdrink                 int64\n",
            "fishmetal                 int64\n",
            "drums                     int64\n",
            "appliance                 int64\n",
            "carparts                  int64\n",
            "cables                    int64\n",
            "other.metal               int64\n",
            "wellies                   int64\n",
            "balloon                   int64\n",
            "bobbins                   int64\n",
            "tyre                      int64\n",
            "gloves                    int64\n",
            "other.rub                 int64\n",
            "jars                      int64\n",
            "bottles                   int64\n",
            "pieces                    int64\n",
            "other.glass               int64\n",
            "woodnat                   int64\n",
            "woodproc                  int64\n",
            "rope                      int64\n",
            "paper                     int64\n",
            "pallets                   int64\n",
            "other.wood                int64\n",
            "clothing                  int64\n",
            "shoes                     int64\n",
            "other.misc                int64\n",
            "totallitter               int64\n",
            "distance                  int64\n",
            "wingspread                int64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Data type check\n",
        "print(\"\\n --- Data Types ---\")\n",
        "print(raw_df.dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.7.5 Checking Category Dictionary - Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " --- Categories ---\n",
            "Plastic: ['bottle', 'sheet', 'bag', 'caps', 'fishline.mono', 'fishline.tang', 'synthrope', 'fishnet', 'cabletie', 'strap', 'nappies', 'crates', 'santowels', 'other.plas']\n",
            "Metal: ['cansfood', 'cansdrink', 'fishmetal', 'drums', 'appliance', 'carparts', 'cables', 'other.metal']\n",
            "Rubber: ['wellies', 'balloon', 'bobbins', 'tyre', 'gloves', 'other.rub']\n",
            "Glass: ['jars', 'bottles', 'pieces', 'other.glass']\n",
            "Plant-Based Materials: ['woodnat', 'woodproc', 'rope', 'paper', 'pallets', 'other.wood']\n",
            "Clothing & Fabrics: ['clothing', 'shoes']\n",
            "Miscellaneous: ['other.misc']\n"
          ]
        }
      ],
      "source": [
        "# Category Dictionary\n",
        "print(\"\\n --- Categories ---\")\n",
        "for key, value in categories.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Marine Litter Categories:\n",
            "- Plastic:\n",
            "  * bottle\n",
            "  * sheet\n",
            "  * bag\n",
            "  * caps\n",
            "  * fishline.mono\n",
            "  * fishline.tang\n",
            "  * synthrope\n",
            "  * fishnet\n",
            "  * cabletie\n",
            "  * strap\n",
            "  * nappies\n",
            "  * crates\n",
            "  * santowels\n",
            "  * other.plas\n",
            "- Metal:\n",
            "  * cansfood\n",
            "  * cansdrink\n",
            "  * fishmetal\n",
            "  * drums\n",
            "  * appliance\n",
            "  * carparts\n",
            "  * cables\n",
            "  * other.metal\n",
            "- Rubber:\n",
            "  * wellies\n",
            "  * balloon\n",
            "  * bobbins\n",
            "  * tyre\n",
            "  * gloves\n",
            "  * other.rub\n",
            "- Glass:\n",
            "  * jars\n",
            "  * bottles\n",
            "  * pieces\n",
            "  * other.glass\n",
            "- Plant-Based Materials:\n",
            "  * woodnat\n",
            "  * woodproc\n",
            "  * rope\n",
            "  * paper\n",
            "  * pallets\n",
            "  * other.wood\n",
            "- Clothing & Fabrics:\n",
            "  * clothing\n",
            "  * shoes\n",
            "- Miscellaneous:\n",
            "  * other.misc\n"
          ]
        }
      ],
      "source": [
        "# Nicer format\n",
        "print(\"\\n Marine Litter Categories:\")\n",
        "for category, items in categories.items():\n",
        "    print(f\"- {category}:\")  # Add a dash and a space\n",
        "    for item in items:\n",
        "        print(f\"  * {item}\")  # Add two spaces and an asterisk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_df = raw_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.8 Saving Cleaned & Processed Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the DataFrame to a new CSV file\n",
        "processed_df.to_csv('data/02_PROCESSED_NEA-Seafloor-Litter.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
