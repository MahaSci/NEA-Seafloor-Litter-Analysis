{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* This notebook documents ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- **Obj**: ....\n",
    " \n",
    "\n",
    "\n",
    "## Inputs\n",
    "* Processed Dataset: `02_PROCESSED_NEA-Seafloor-Litter.csv`. \n",
    "\n",
    "    This served as the initial data source.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* .....\n",
    "\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* .....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.1 Research and experiment with the application of data analytics tools, technologies, and methodologies:\n",
    "Explain why you chose specific tools (e.g., comparing different Python libraries like Matplotlib vs. Plotly for visualisation). Detail in Jupyter any experiments with new tools or methodologies, showing code snippets and explaining their results. Include version control commits that show progressive experimentation and refinements of your work. Incorporate new tools/technologies you've researched (such as a new library). The project would show evidence of trial, adaptation, and application in a meaningful way. Focus on challenges encountered and solutions found, explaining how you adapted to using them in the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate proper file access, the working directory is to be adjusted to its parent directory\n",
    "* os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mahahussain/Desktop/NEA-Seafloor-Litter-Analysis/NEA-Seafloor-Litter-Analysis/jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the parent of the current directory the new current directory:\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mahahussain/Desktop/NEA-Seafloor-Litter-Analysis/NEA-Seafloor-Litter-Analysis'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this section:\n",
    "- Introduction to the key statistical concepts used throughout the analysis.\n",
    "- #TODO\n",
    "-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introduction to Core Statistical Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During data analysis, we want to be able to describe and interpret our dataset. We can do so by utilising statistical concepts that help us identify patterns and draw conclusions.\n",
    "\n",
    "Summary statistics summarise the mean, median, mode, standard deviation and min/max values in one statistic.\n",
    "\n",
    "---\n",
    "\n",
    "1. `MEAN:` The mean is a statistical tool used to find the average of a set of given numbers. \n",
    "\n",
    "- To calculate it, sum all of the values in the dataset and divide that sum by the count. We can use np.mean() to do so.\n",
    "\n",
    "    - Sum: Total amount of the addition of all datapoints in the given numerical column.\n",
    "    - Count: Total number of all datapoints in the set.\n",
    "\n",
    "This is an important foundational principle for data analysis because it provides a measure of central tendency which represents the typical value in a given data column. This allows us to compare averages across different datasets or subgroups within a given dataset. The mean can also be used to examine the change in mean over an extended period of time to view shifts and trends in the data.\n",
    "\n",
    "Note: It is important to note, however, that the mean is sensitive to extreme outliers. In datasets that have highly skewed distribution, the mean may not accurately represent a typical datapoint. Hence, we must investigate the data in conjunctions with other statistical measures.\n",
    "\n",
    "---\n",
    "\n",
    "2. `MEDIAN:` The median is a statistical tool used to find the middle value of an ordered set of given numbers. \n",
    "\n",
    "- To calculate it, you first need to order the datapoints in ascending order.\n",
    "\n",
    "    - If you have an odd number of values, the median is simply the middle number.\n",
    "    - If you have an even number of values, the median is the averaege of the two middle numbers.\n",
    "\n",
    "    We can use np.median() to do so.\n",
    "\n",
    "This is an important foundational principle for data analysis because it helps to identify a typical value of the dataset - 50% of the data is above this point and 50% of the data is below this point.\n",
    "\n",
    "Note: Unlike the mean, the median is not affected by extreme outliers, therefore it is a better measure of central tendency for a dataset with a skewed distribution. If the mean and median values are closely aligned, the data is likely symmetrical with a normal distribution, else, if they are not aligned, the data is likely skewed.\n",
    "\n",
    "---\n",
    "\n",
    "3. `MODE:` The mode is a statistical tool used to find the most frequent value in the given datapoints.\n",
    "\n",
    "- To calculate it, simply count the number of times each value appears in the data.\n",
    "\n",
    "We can use scipy.stats.mode() to do so on a numpy array.\n",
    "\n",
    "This is an important foundational principle for data analysis because because it allows us to identify the most common / popular value in our data. It is especially helpful for analysing categorical types, such as types of litter.\n",
    "    \n",
    "---\n",
    "\n",
    "4. `STANDARD DEVIATION:` Standard deviation is a statistical tool used to measure how spread out the numbers in a dataset are.\n",
    "\n",
    "- To calculate it you can use: np.std(data)\n",
    "- Since we are using DataFrames we can use df.std() from Pandas.\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{\\sum (x_i - \\mu)^2}{N}}\n",
    "$$\n",
    "\n",
    "This is an important foundational principle for data analysis because it tells us how far out the data is from the mean value. \n",
    "\n",
    "- Small standard deviation: The datapoints are close to the average.\n",
    "- Large standard deviation: The datappoints are far from the average.\n",
    "\n",
    "---\n",
    "5. `HYPOTHESIS TESTING:` Hypothesis testing is a statistical concept used to check if our guesses about the data are likely to be true.\n",
    "\n",
    "- It involves setting up a \"null hypothesis\" (a statement we want to test) and then using statistical tests to see if the data provides enough evidence to reject it.\n",
    "\n",
    "This is an important foundational principle for data analysis because it allows us to check if observerd patterns are either likely to be due to chance or if they are statistically significant.\n",
    "\n",
    "---\n",
    "6. `BASIC PROBABILITY:` Probability is a statistical concept used to understand how likely it is that something will happen.\n",
    "\n",
    "- It's expressed as a number between 0 and 1 (or as a percentage). 0 means it's impossible, and 1 (or 100%) means it's certain.\n",
    "\n",
    "This is an important foundational principle for data analysis because it helps us understand the changes of different outcomes. For example, if we want to know the probability of finding a certain type of litter in a specifc area, based on the historical information, we can use probability to understand how likely that is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Statistical Summary for Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides a statistical overview of the numerical data, calculating and interpreting descriptive measures such as mean, median, and standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the processed dataset\n",
    "df = pd.read_csv('data/02_PROCESSED_NEA-Seafloor-Litter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survey</th>\n",
       "      <th>cruise</th>\n",
       "      <th>area</th>\n",
       "      <th>station</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>bottle</th>\n",
       "      <th>sheet</th>\n",
       "      <th>bag</th>\n",
       "      <th>...</th>\n",
       "      <th>other.wood</th>\n",
       "      <th>clothing</th>\n",
       "      <th>shoes</th>\n",
       "      <th>other.misc</th>\n",
       "      <th>totallitter</th>\n",
       "      <th>distance</th>\n",
       "      <th>wingspread</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBTS</td>\n",
       "      <td>CIRO 9/92</td>\n",
       "      <td>Greater North Sea</td>\n",
       "      <td>1</td>\n",
       "      <td>51.738333</td>\n",
       "      <td>1.753333</td>\n",
       "      <td>1992-08-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3794</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IBTS</td>\n",
       "      <td>CIRO 9/92</td>\n",
       "      <td>Greater North Sea</td>\n",
       "      <td>2</td>\n",
       "      <td>51.601667</td>\n",
       "      <td>2.796667</td>\n",
       "      <td>1992-08-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3918</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBTS</td>\n",
       "      <td>CIRO 9/92</td>\n",
       "      <td>Greater North Sea</td>\n",
       "      <td>3</td>\n",
       "      <td>51.823333</td>\n",
       "      <td>3.643333</td>\n",
       "      <td>1992-08-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3624</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IBTS</td>\n",
       "      <td>CIRO 9/92</td>\n",
       "      <td>Greater North Sea</td>\n",
       "      <td>4</td>\n",
       "      <td>52.823333</td>\n",
       "      <td>2.760000</td>\n",
       "      <td>1992-08-16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3642</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IBTS</td>\n",
       "      <td>CIRO 9/92</td>\n",
       "      <td>Greater North Sea</td>\n",
       "      <td>5</td>\n",
       "      <td>52.685000</td>\n",
       "      <td>3.411667</td>\n",
       "      <td>1992-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3791</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  survey     cruise               area  station   Latitude  Longitude  \\\n",
       "0   IBTS  CIRO 9/92  Greater North Sea        1  51.738333   1.753333   \n",
       "1   IBTS  CIRO 9/92  Greater North Sea        2  51.601667   2.796667   \n",
       "2   IBTS  CIRO 9/92  Greater North Sea        3  51.823333   3.643333   \n",
       "3   IBTS  CIRO 9/92  Greater North Sea        4  52.823333   2.760000   \n",
       "4   IBTS  CIRO 9/92  Greater North Sea        5  52.685000   3.411667   \n",
       "\n",
       "         date  bottle  sheet  bag  ...  other.wood  clothing  shoes  \\\n",
       "0  1992-08-14       0      0    1  ...           0         0      0   \n",
       "1  1992-08-15       0      0    1  ...           0         0      0   \n",
       "2  1992-08-15       0      0    0  ...           0         0      0   \n",
       "3  1992-08-16       0      0    1  ...           0         0      0   \n",
       "4  1992-08-16       1      0    1  ...           0         0      0   \n",
       "\n",
       "   other.misc  totallitter  distance  wingspread  year  month  day  \n",
       "0           0            1      3794           0  1992      8   14  \n",
       "1           0            2      3918           0  1992      8   15  \n",
       "2           0            1      3624           0  1992      8   15  \n",
       "3           0            1      3642           0  1992      8   16  \n",
       "4           0            2      3791           0  1992      8   16  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: totallitter\n",
      "count    4307.000000\n",
      "mean        2.451823\n",
      "std         6.979546\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         1.000000\n",
      "75%         3.000000\n",
      "max       271.000000\n",
      "Name: totallitter, dtype: float64\n",
      "Median: 1.0\n",
      "Mode: 0\n",
      "\n",
      "\n",
      "Column: distance\n",
      "count     4307.000000\n",
      "mean      2071.758068\n",
      "std       2212.877810\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%       2951.000000\n",
      "75%       3696.000000\n",
      "max      13208.000000\n",
      "Name: distance, dtype: float64\n",
      "Median: 2951.0\n",
      "Mode: 0\n",
      "\n",
      "\n",
      "Column: wingspread\n",
      "count    4307.000000\n",
      "mean        3.038078\n",
      "std         4.618863\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         4.000000\n",
      "75%         4.000000\n",
      "max        22.000000\n",
      "Name: wingspread, dtype: float64\n",
      "Median: 4.0\n",
      "Mode: 4\n",
      "\n",
      "\n",
      "Column: year\n",
      "count    4307.000000\n",
      "mean     2007.981658\n",
      "std         7.048329\n",
      "min      1992.000000\n",
      "25%      2005.000000\n",
      "50%      2011.000000\n",
      "75%      2013.000000\n",
      "max      2015.000000\n",
      "Name: year, dtype: float64\n",
      "Median: 2011.0\n",
      "Mode: 2011\n",
      "\n",
      "\n",
      "Column: month\n",
      "count    4307.000000\n",
      "mean        6.879731\n",
      "std         2.710771\n",
      "min         2.000000\n",
      "25%         3.000000\n",
      "50%         8.000000\n",
      "75%         9.000000\n",
      "max        12.000000\n",
      "Name: month, dtype: float64\n",
      "Median: 8.0\n",
      "Mode: 8\n",
      "\n",
      "\n",
      "Column: day\n",
      "count    4307.000000\n",
      "mean       17.098677\n",
      "std         8.281551\n",
      "min         1.000000\n",
      "25%        11.000000\n",
      "50%        18.000000\n",
      "75%        24.000000\n",
      "max        31.000000\n",
      "Name: day, dtype: float64\n",
      "Median: 18.0\n",
      "Modes: 20, 22\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Columns that we want to describe\n",
    "cols_to_describe = ['totallitter', 'distance', 'wingspread', 'year', 'month', 'day']\n",
    "\n",
    "def describe_columns(df, cols):\n",
    "    for col in cols:\n",
    "        print(f\"Column: {col}\")\n",
    "        print(df[col].describe())  # Get the usual describe() output\n",
    "\n",
    "        # Add median\n",
    "        median = df[col].median()\n",
    "        print(f\"Median: {median}\")\n",
    "\n",
    "        # Add mode (handling potential multiple modes)\n",
    "        mode = df[col].mode()\n",
    "        if len(mode) == 1:\n",
    "            print(f\"Mode: {mode[0]}\")\n",
    "        else:\n",
    "            print(f\"Modes: {', '.join(map(str, mode.tolist()))}\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "describe_columns(df, cols_to_describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Insights from Descriptive Statistics (Numerical)\n",
    "\n",
    "**1. Marine Litter Abundance ('totallitter'):**\n",
    "\n",
    "* The mean litter count per survey is 2.45, with a high standard deviation of 6.98, indicating significant variability.\n",
    "* A median of 1 and mode of 0 suggest that most surveys recorded very low or no litter.\n",
    "* The maximum count of 271 highlights the presence of extreme accumulation events, contributing to data skewness.\n",
    "\n",
    "**2. Survey Distance ('distance'):**\n",
    "\n",
    "* The mean survey distance is 2071.76, with a large standard deviation of 2212.88, reflecting substantial variation in survey effort.\n",
    "* The median distance of 2951 exceeds the mean, and a mode of 0 indicates numerous surveys with minimal or no distance covered.\n",
    "* This suggests a negatively skewed distribution, with potential clustering of surveys at shorter distances.\n",
    "\n",
    "**3. Wingspread Measurements ('wingspread'):**\n",
    "\n",
    "* The mean wingspread is 3.04 (std 4.62), showing considerable variability.\n",
    "* The median and mode of 4 indicate a common measurement value.\n",
    "* The 25th percentile of 0 suggests a significant proportion of surveys did not record wingspread data.\n",
    "\n",
    "**4. Temporal Survey Distribution (Year, Month, Day):**\n",
    "\n",
    "* Surveys span 1992-2015 (mean 2008, std 7.04), showing a wide temporal range.\n",
    "* The median and mode year of 2011 suggest a concentration of survey activity around this year.\n",
    "* Surveys are distributed throughout the year (mean month 6.88), with a median and mode month of 8 (August), indicating peak survey activity during this month.\n",
    "* The mean day is 17, with a median of 18 and modes of 20 and 22, indicating a slight tendency for surveys in the latter half of the month.\n",
    "\n",
    "**General Observations:**\n",
    "\n",
    "* High standard deviations across 'totallitter', 'distance', and 'wingspread' suggest substantial data variability, warranting further investigation into influencing factors.\n",
    "* Skewness in 'totallitter' and 'distance' suggests the presence of outliers and non-normal distributions, requiring appropriate statistical handling.\n",
    "* Temporal data, including medians and modes, provides valuable context for understanding long-term and seasonal trends in marine litter distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contextualising the AI summary \n",
    "While the descriptive statistics provide a quantitative overview, it is crucial to acknowledge the inherent limitations and potential biases in the data. The observed variability in litter counts and survey distances may reflect not only genuine environmental fluctuations but also variations in survey methodologies, environmental conditions, and observer biases. Furthermore, the temporal distribution of surveys, while seemingly uniform, might be influenced by logistical constraints and funding cycles, which could impact the representativeness of the data. Therefore, the interpretation of these statistics must be approached with a nuanced understanding of the data collection process and its potential limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Statistical Summary for Categorical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** To perform a descriptive analysis of categorical variables within the dataset.\n",
    "\n",
    "**Methodology:**\n",
    "Utilise frequency counts to determine the occurrence of each unique value within selected categorical columns.\n",
    "Examine the cardinality of these columns, which refers to the number of unique categories present.\n",
    "\n",
    "**Purpose:**\n",
    "To provide a comprehensive understanding of the distribution and diversity of categorical variables.\n",
    "To contribute to a broader understanding of the dataset's composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Mode of Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_mode = ['year', 'month', 'day', 'survey', 'cruise', 'area', 'station']\n",
    "\n",
    "def find_modes(df, cols_for_mode):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Using value_counts() to examine frequency of each unique value in select columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical data, representing distinct groups or classifications, requires different statistical approaches compared to numerical data. We will utilise frequency coubts to determine the unique occurence of each value within selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Value count for year:\n",
      "year\n",
      "2011    640\n",
      "2010    570\n",
      "2014    500\n",
      "2013    484\n",
      "2012    416\n",
      "2015    277\n",
      "2009    163\n",
      "1994    150\n",
      "1992    149\n",
      "1993    146\n",
      "2008    111\n",
      "2000     92\n",
      "2005     86\n",
      "1999     74\n",
      "1998     74\n",
      "1997     74\n",
      "1995     74\n",
      "1996     72\n",
      "2007     31\n",
      "2003     28\n",
      "2006     27\n",
      "2004     25\n",
      "2001     22\n",
      "2002     22\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for month:\n",
      "month\n",
      "8     1085\n",
      "9      811\n",
      "3      782\n",
      "7      616\n",
      "11     321\n",
      "2      296\n",
      "6      192\n",
      "10     125\n",
      "4       59\n",
      "12      19\n",
      "5        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for day:\n",
      "day\n",
      "20    219\n",
      "22    219\n",
      "17    197\n",
      "18    190\n",
      "19    190\n",
      "21    189\n",
      "28    173\n",
      "25    172\n",
      "23    169\n",
      "26    168\n",
      "16    164\n",
      "27    161\n",
      "24    159\n",
      "13    136\n",
      "15    134\n",
      "1     128\n",
      "12    119\n",
      "14    117\n",
      "2     111\n",
      "30    110\n",
      "9     110\n",
      "29    110\n",
      "3     107\n",
      "8     102\n",
      "7     101\n",
      "5     101\n",
      "6      99\n",
      "10     98\n",
      "4      97\n",
      "11     97\n",
      "31     60\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for survey:\n",
      "survey\n",
      "IBTS                 1575\n",
      "NWGFS                 566\n",
      "Q1SW_with Blinder     496\n",
      "7DBTS                 444\n",
      "Q1SW_No Blinder       408\n",
      "CSEMP                 365\n",
      "Q4SW                  220\n",
      "Q1SW                  138\n",
      "MEMFISH                95\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for cruise:\n",
      "cruise\n",
      "CEND 4/15     277\n",
      "Cend04/14     192\n",
      "Cend5/11      175\n",
      "Cend6/10      172\n",
      "CEND 02/13    142\n",
      "Cend 18/13    118\n",
      "CEND 13/12    118\n",
      "CEND 15/11    116\n",
      "Cend 18/14    112\n",
      "CEND 14/10    112\n",
      "CEND 15/12    108\n",
      "CEND4/11       95\n",
      "Cend 12/10     94\n",
      "CEND 11/12     93\n",
      "CEND 15/14     91\n",
      "CEND 13/10     90\n",
      "CEND 12/09     90\n",
      "Cend 15/13     89\n",
      "CEND 14/11     88\n",
      "CEND 12/13     85\n",
      "CEND5/12       84\n",
      "CEND 13/11     81\n",
      "Cend 17/14     81\n",
      "CEND 17/10     79\n",
      "CIRO 6/00      75\n",
      "CIRO 11/94     75\n",
      "CIRO 9/94      75\n",
      "CIRO 9/92      75\n",
      "CIRO 7/95      74\n",
      "CIRO 7/97      74\n",
      "CIRO 8/93      74\n",
      "CIRO 6/99      74\n",
      "CIRO 11/92     74\n",
      "CEND 15/08     74\n",
      "CIRO 4/98      74\n",
      "Cend 17/09     73\n",
      "CIRO 10/93     72\n",
      "CIRO 8/96      72\n",
      "CEND 19/11     68\n",
      "CEND 13/05     57\n",
      "Cend11/13      50\n",
      "CSEMP2008      37\n",
      "CSEMP2007      31\n",
      "CSEMP2005      29\n",
      "CSEMP2003      28\n",
      "CSEMP2006      27\n",
      "CSEMP2004      25\n",
      "Cend14/14      24\n",
      "CESEMP2010     23\n",
      "CSEMP2002      22\n",
      "CSEMP2001      22\n",
      "CSEMP2000      17\n",
      "CSEMP2011      17\n",
      "CEND9/12       13\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for area:\n",
      "area\n",
      "Greater North Sea                 2241\n",
      "Celtic Seas                       1913\n",
      "Celtic Seas, Greater North Sea     143\n",
      "Unknown                             10\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for station:\n",
      "station\n",
      "0      1031\n",
      "7        39\n",
      "22       38\n",
      "47       37\n",
      "53       37\n",
      "       ... \n",
      "142       1\n",
      "288       1\n",
      "108       1\n",
      "290       1\n",
      "287       1\n",
      "Name: count, Length: 320, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def get_value_counts(data_frame):\n",
    "   \"\"\"\n",
    "    This function shows the frequency of each category for the following columns:\n",
    "    1. year, month, day\n",
    "    2. survey\n",
    "    3. cruise\n",
    "    4. area\n",
    "    5. station\n",
    "\n",
    "   It provides the count of each unique category for the specified columns.\n",
    "\n",
    "    Args: data_frame: The data frame to inspect.\n",
    "    \"\"\"\n",
    "   \n",
    "   # Create a list of the columns to inspect\n",
    "   columns_to_check = ['year', 'month', 'day', 'survey', 'cruise', 'area', 'station']\n",
    "\n",
    "   # For every col in the list, print its title and its value counts\n",
    "   for col in columns_to_check:\n",
    "      print(f\"\\n Value count for {col}:\")\n",
    "      print(data_frame[col].value_counts())\n",
    "    \n",
    "get_value_counts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AI Insights for Value Counts**\n",
    "   **Year Distribution**\n",
    "  - The data spans several years, with 2011 having the highest count (640) and 2002 the lowest (22).\n",
    "  - The year range covers both recent and older periods, with a noticeable decrease in data points as you go further back in time.\n",
    "  - The years 2011, 2010, and 2014 dominate, indicating that more data was collected in those years.\n",
    "\n",
    "  **Survey Distribution**\n",
    "  - **IBTS** survey has the highest count (1575), significantly outpacing all other surveys.\n",
    "  - The next largest survey is **NWGFS** (566), followed by **Q1SW_with Blinder** (496), suggesting these surveys might be more extensive or more frequently conducted.\n",
    "  - Other surveys, such as **MEMFISH** (95) and **Q1SW** (138), have significantly lower counts, indicating they might be more specialised or less frequent.\n",
    "\n",
    "  **Cruise Distribution**\n",
    "  - **CEND 4/15** has the highest cruise count (277), followed by other CEND series cruises (e.g., **Cend04/14**, **Cend5/11**), which seem to be relatively frequent (counts between 90 and 200).\n",
    "  - Several cruises with **CIRO** prefixes (e.g., **CIRO 6/00**, **CIRO 11/94**) have counts of 75, indicating they were conducted on a few key occasions.\n",
    "  - Some cruises like **CSEMP2008** (37) and **CSEMP2007** (31) show smaller numbers, likely due to being more specific or infrequent surveys.\n",
    "\n",
    "  **Area Distribution**\n",
    "  - Two main areas: **Greater North Sea** (2241) and **Celtic Seas** (1792).\n",
    "  - **Greater North Sea** has a significantly higher count, indicating it may be a more frequently surveyed or larger region compared to the **Celtic Seas**.\n",
    "  \n",
    "  **Station Distribution**\n",
    "  - Station values are widely distributed:\n",
    "    - **Station 0** has the highest count (1031), indicating it's a primary or central location.\n",
    "    - Other stations have significantly lower counts, with some stations (like 290, 142, 292) having only 1 count, suggesting they represent rare or specific locations.\n",
    "\n",
    "\n",
    "#### **Observations**\n",
    "From the analysis, it's clear that the data collection has been much more concentrated in recent years, particularly around 2010-2014. The dominance of the **IBTS** survey suggests it's the primary source of the data, with a smaller set of other surveys contributing. The variety in the number of cruises and the regions surveyed shows that the data may come from both regular, extensive surveys (Greater North Sea) and more targeted studies in specific areas. It also seems that Station 0 is the central location for data collection, while others are used sparingly or for more niche observations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Using nunique() to Check Cardinality (Number of Unique Values of Spec. Column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique years: 24\n",
      "Number of unique months: 11\n",
      "Number of unique days: 31\n",
      "Number of unique surveys: 9\n",
      "Number of unique cruises: 54\n",
      "Number of unique areas: 4\n",
      "Number of unique stations: 320\n"
     ]
    }
   ],
   "source": [
    "def get_cardinality(data_frame):\n",
    "   \"\"\"\n",
    "    This function shows the count of distinct categories for the following columns:\n",
    "    1. year\n",
    "    2. survey\n",
    "    3. cruise\n",
    "    4. area\n",
    "    5. station\n",
    "\n",
    "    Args: data_frame: The data frame to inspect.\n",
    "    \"\"\"\n",
    "       \n",
    "   # Create a list of the columns to inspect\n",
    "   cols = ['year', 'month', 'day', 'survey', 'cruise', 'area', 'station']\n",
    "\n",
    "   # For every col in the list, print its title and its value counts\n",
    "   for col in cols:\n",
    "      unqiue_count = data_frame[col].nunique()\n",
    "      print(f\"Number of unique {col}s: {unqiue_count}\") \n",
    "  \n",
    "    \n",
    "get_cardinality(df)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique areas: ['Greater North Sea' 'Celtic Seas' 'Celtic Seas, Greater North Sea'\n",
      " 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "# list of unique areas\n",
    "unique_areas = df['area'].unique()\n",
    "print(f\"\\nUnique areas: {unique_areas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AI Insights for Unique Category Counts**\n",
    "**Year Distribution**\n",
    "- The data spans across **24 unique years**, indicating a broad temporal range of data collection.\n",
    "- The presence of multiple years reflects long-term monitoring, with a focus on both recent and older data points.\n",
    "\n",
    "**Survey Distribution**\n",
    "- **9 unique surveys** are represented, showcasing a diverse range of research efforts.\n",
    "- The presence of multiple surveys suggests a variety of research methodologies and objectives, capturing different aspects of the dataset.\n",
    "\n",
    "**Cruise Distribution**\n",
    "- The data covers **54 unique cruises**, indicating a diverse set of research trips or expeditions.\n",
    "- The variety in cruises suggests both extensive and more specific sampling events across various periods.\n",
    "\n",
    "**Area Distribution**\n",
    "- The data is focused on **2 unique areas**, likely representing two primary regions of interest.\n",
    "- These areas may have distinct environmental conditions, offering opportunities for comparison and regional analysis.\n",
    "\n",
    "**Station Distribution**\n",
    "- The dataset includes data from **320 unique stations**, reflecting a detailed geographical distribution.\n",
    "- The large number of stations suggests a comprehensive sampling effort, providing rich spatial data for analysis.\n",
    "\n",
    "#### **Observations**\n",
    "- The data spans a significant time period with 24 years of collected data.\n",
    "- The variety in the number of surveys, cruises, and stations shows a comprehensive and diverse data collection effort, capturing both broad trends and specific research events.\n",
    "- The focus on two primary areas highlights regional interest, while the extensive number of stations offers a high level of detail and spatial resolution in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Additional Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variability, outlers, discussion of data limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Intro to Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why univariate analysis for distrubiotn\n",
    "why distributions\n",
    "why chose to look at x vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analysis of Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analysis of Categorical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualisation Insights & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 : Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Intro to Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Categorical vs. Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Categorical vs. Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Numerical vs. Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualisation Insights & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 : Insights and Interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Summary of Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Discussion of Implications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
