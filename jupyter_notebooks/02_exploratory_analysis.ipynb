{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* This notebook documents ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- **Obj**: ....\n",
    " \n",
    "\n",
    "\n",
    "## Inputs\n",
    "* Processed Dataset: `02_PROCESSED_NEA-Seafloor-Litter.csv`. \n",
    "\n",
    "    This served as the initial data source.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* .....\n",
    "\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* .....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.1 Research and experiment with the application of data analytics tools, technologies, and methodologies:\n",
    "Explain why you chose specific tools (e.g., comparing different Python libraries like Matplotlib vs. Plotly for visualisation). Detail in Jupyter any experiments with new tools or methodologies, showing code snippets and explaining their results. Include version control commits that show progressive experimentation and refinements of your work. Incorporate new tools/technologies you've researched (such as a new library). The project would show evidence of trial, adaptation, and application in a meaningful way. Focus on challenges encountered and solutions found, explaining how you adapted to using them in the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this section:\n",
    "- Introduction to the key statistical concepts used throughout the analysis.\n",
    "- #TODO\n",
    "-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introduction to Core Statistical Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During data analysis, we want to be able to describe and interpret our dataset. We can do so by utilising statistical concepts that help us identify patterns and draw conclusions.\n",
    "\n",
    "Summary statistics summarise the mean, median, mode, standard deviation and min/max values in one statistic.\n",
    "\n",
    "---\n",
    "\n",
    "1. `MEAN:` The mean is a statistical tool used to find the average of a set of given numbers. \n",
    "\n",
    "- To calculate it, sum all of the values in the dataset and divide that sum by the count. We can use np.mean() to do so.\n",
    "\n",
    "    - Sum: Total amount of the addition of all datapoints in the given numerical column.\n",
    "    - Count: Total number of all datapoints in the set.\n",
    "\n",
    "This is an important foundational principle for data analysis because it provides a measure of central tendency which represents the typical value in a given data column. This allows us to compare averages across different datasets or subgroups within a given dataset. The mean can also be used to examine the change in mean over an extended period of time to view shifts and trends in the data.\n",
    "\n",
    "Note: It is important to note, however, that the mean is sensitive to extreme outliers. In datasets that have highly skewed distribution, the mean may not accurately represent a typical datapoint. Hence, we must investigate the data in conjunctions with other statistical measures.\n",
    "\n",
    "---\n",
    "\n",
    "2. `MEDIAN:` The median is a statistical tool used to find the middle value of an ordered set of given numbers. \n",
    "\n",
    "- To calculate it, you first need to order the datapoints in ascending order.\n",
    "\n",
    "    - If you have an odd number of values, the median is simply the middle number.\n",
    "    - If you have an even number of values, the median is the averaege of the two middle numbers.\n",
    "\n",
    "    We can use np.median() to do so.\n",
    "\n",
    "This is an important foundational principle for data analysis because it helps to identify a typical value of the dataset - 50% of the data is above this point and 50% of the data is below this point.\n",
    "\n",
    "Note: Unlike the mean, the median is not affected by extreme outliers, therefore it is a better measure of central tendency for a dataset with a skewed distribution. If the mean and median values are closely aligned, the data is likely symmetrical with a normal distribution, else, if they are not aligned, the data is likely skewed.\n",
    "\n",
    "---\n",
    "\n",
    "3. `MODE:` The mode is a statistical tool used to find the most frequent value in the given datapoints.\n",
    "\n",
    "- To calculate it, simply count the number of times each value appears in the data.\n",
    "\n",
    "We can use scipy.stats.mode() to do so on a numpy array.\n",
    "\n",
    "This is an important foundational principle for data analysis because because it allows us to identify the most common / popular value in our data. It is especially helpful for analysing categorical types, such as types of litter.\n",
    "    \n",
    "---\n",
    "\n",
    "4. `STANDARD DEVIATION:` Standard deviation is a statistical tool used to measure how spread out the numbers in a dataset are.\n",
    "\n",
    "- To calculate it you can use: np.std(data) when N=1, np.std(data, ddof=1) for N-1.\n",
    "- Since we are using DataFrames we can use df.std() from Pandas.\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{\\sum (x_i - \\mu)^2}{N}}\n",
    "$$\n",
    "\n",
    "This is an important foundational principle for data analysis because it tells us how far out the data is from the mean value. \n",
    "\n",
    "- Small standard deviation: The datapoints are close to the average.\n",
    "- Large standard deviation: The datappoints are far from the average.\n",
    "\n",
    "---\n",
    "5. `HYPOTHESIS TESTING:` Hypothesis testing is a statistical concept used to check if our guesses about the data are likely to be true.\n",
    "\n",
    "- It involves setting up a \"null hypothesis\" (a statement we want to test) and then using statistical tests to see if the data provides enough evidence to reject it.\n",
    "\n",
    "This is an important foundational principle for data analysis because it allows us to check if observerd patterns are either likely to be due to chance or if they are statistically significant.\n",
    "\n",
    "---\n",
    "6. `BASIC PROBABILITY:` The median is a statistical concept used to understand how likely it is that something will happen.\n",
    "\n",
    "- It's expressed as a number between 0 and 1 (or as a percentage). 0 means it's impossible, and 1 (or 100%) means it's certain.\n",
    "\n",
    "This is an important foundational principle for data analysis because it helps us understand the changes of different outcomes. For example, if we want to know the probability of finding a certain type of litter in a specifc area, based on the historical information, we can use probability to understand how likely that is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Statistical Summary for Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calc and present descirptive stats w/ df describe\n",
    "ai summary of findsings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Statistical Summary for Categorical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freq. counts and prooprtions w/ value coutns.\n",
    "ai sumamries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using value_counts() to examine frequency of each unique value in select columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_counts(data_frame):\n",
    "   \"\"\"\n",
    "    This function shows the frequency of each category for the following columns:\n",
    "    1. year\n",
    "    2. survey\n",
    "    3. cruise\n",
    "    4. area\n",
    "    5. station\n",
    "\n",
    "   It provides the count of each unique category for the specified columns.\n",
    "\n",
    "    Args: data_frame: The data frame to inspect.\n",
    "    \"\"\"\n",
    "   \n",
    "   # Create a list of the columns to inspect\n",
    "   columns_to_check = ['year', 'survey', 'cruise', 'area', 'station']\n",
    "\n",
    "   # For every col in the list, print its title and its value counts\n",
    "   for col in columns_to_check:\n",
    "      print(f\"\\n Value count for {col}:\")\n",
    "      print(data_frame[col].value_counts())\n",
    "    \n",
    "get_value_counts(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AI Insights for Value Counts**\n",
    "   **Year Distribution**\n",
    "  - The data spans several years, with 2011 having the highest count (640) and 2002 the lowest (22).\n",
    "  - The year range covers both recent and older periods, with a noticeable decrease in data points as you go further back in time.\n",
    "  - The years 2011, 2010, and 2014 dominate, indicating that more data was collected in those years.\n",
    "\n",
    "  **Survey Distribution**\n",
    "  - **IBTS** survey has the highest count (1575), significantly outpacing all other surveys.\n",
    "  - The next largest survey is **NWGFS** (566), followed by **Q1SW_with Blinder** (496), suggesting these surveys might be more extensive or more frequently conducted.\n",
    "  - Other surveys, such as **MEMFISH** (95) and **Q1SW** (138), have significantly lower counts, indicating they might be more specialised or less frequent.\n",
    "\n",
    "  **Cruise Distribution**\n",
    "  - **CEND 4/15** has the highest cruise count (277), followed by other CEND series cruises (e.g., **Cend04/14**, **Cend5/11**), which seem to be relatively frequent (counts between 90 and 200).\n",
    "  - Several cruises with **CIRO** prefixes (e.g., **CIRO 6/00**, **CIRO 11/94**) have counts of 75, indicating they were conducted on a few key occasions.\n",
    "  - Some cruises like **CSEMP2008** (37) and **CSEMP2007** (31) show smaller numbers, likely due to being more specific or infrequent surveys.\n",
    "\n",
    "  **Area Distribution**\n",
    "  - Two main areas: **Greater North Sea** (2241) and **Celtic Seas** (1792).\n",
    "  - **Greater North Sea** has a significantly higher count, indicating it may be a more frequently surveyed or larger region compared to the **Celtic Seas**.\n",
    "  \n",
    "  **Station Distribution**\n",
    "  - Station values are widely distributed:\n",
    "    - **Station 0** has the highest count (1031), indicating it's a primary or central location.\n",
    "    - Other stations have significantly lower counts, with some stations (like 290, 142, 292) having only 1 count, suggesting they represent rare or specific locations.\n",
    "\n",
    "\n",
    "#### **Observations**\n",
    "From the analysis, it's clear that the data collection has been much more concentrated in recent years, particularly around 2010-2014. The dominance of the **IBTS** survey suggests it's the primary source of the data, with a smaller set of other surveys contributing. The variety in the number of cruises and the regions surveyed shows that the data may come from both regular, extensive surveys (Greater North Sea) and more targeted studies in specific areas. It also seems that Station 0 is the central location for data collection, while others are used sparingly or for more niche observations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nunique() to Check Cardinality (Number of Unique Values of Spec. Column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique years: 24\n",
      "Number of unique surveys: 9\n",
      "Number of unique cruises: 54\n",
      "Number of unique areas: 3\n",
      "Number of unique stations: 320\n"
     ]
    }
   ],
   "source": [
    "def get_cardinality(data_frame):\n",
    "   \"\"\"\n",
    "    This function shows the count of distinct categories for the following columns:\n",
    "    1. year\n",
    "    2. survey\n",
    "    3. cruise\n",
    "    4. area\n",
    "    5. station\n",
    "\n",
    "    Args: data_frame: The data frame to inspect.\n",
    "    \"\"\"\n",
    "       \n",
    "   # Create a list of the columns to inspect\n",
    "   cols = ['year', 'survey', 'cruise', 'area', 'station']\n",
    "\n",
    "   # For every col in the list, print its title and its value counts\n",
    "   for col in cols:\n",
    "      unqiue_count = data_frame[col].nunique()\n",
    "      print(f\"Number of unique {col}s: {unqiue_count}\") \n",
    "  \n",
    "    \n",
    "get_cardinality(raw_df)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AI Insights for Unique Category Counts**\n",
    "**Year Distribution**\n",
    "- The data spans across **24 unique years**, indicating a broad temporal range of data collection.\n",
    "- The presence of multiple years reflects long-term monitoring, with a focus on both recent and older data points.\n",
    "\n",
    "**Survey Distribution**\n",
    "- **9 unique surveys** are represented, showcasing a diverse range of research efforts.\n",
    "- The presence of multiple surveys suggests a variety of research methodologies and objectives, capturing different aspects of the dataset.\n",
    "\n",
    "**Cruise Distribution**\n",
    "- The data covers **54 unique cruises**, indicating a diverse set of research trips or expeditions.\n",
    "- The variety in cruises suggests both extensive and more specific sampling events across various periods.\n",
    "\n",
    "**Area Distribution**\n",
    "- The data is focused on **2 unique areas**, likely representing two primary regions of interest.\n",
    "- These areas may have distinct environmental conditions, offering opportunities for comparison and regional analysis.\n",
    "\n",
    "**Station Distribution**\n",
    "- The dataset includes data from **320 unique stations**, reflecting a detailed geographical distribution.\n",
    "- The large number of stations suggests a comprehensive sampling effort, providing rich spatial data for analysis.\n",
    "\n",
    "#### **Observations**\n",
    "- The data spans a significant time period with 24 years of collected data.\n",
    "- The variety in the number of surveys, cruises, and stations shows a comprehensive and diverse data collection effort, capturing both broad trends and specific research events.\n",
    "- The focus on two primary areas highlights regional interest, while the extensive number of stations offers a high level of detail and spatial resolution in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Additional Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variability, outlers, discussion of data limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Intro to Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why univariate analysis for distrubiotn\n",
    "why distributions\n",
    "why chose to look at x vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analysis of Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analysis of Categorical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualisation Insights & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 : Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Intro to Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Categorical vs. Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Categorical vs. Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Numerical vs. Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualisation Insights & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 : Insights and Interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Summary of Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Discussion of Implications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
