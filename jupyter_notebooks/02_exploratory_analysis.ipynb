{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* This notebook documents ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- **Obj**: ....\n",
    " \n",
    "\n",
    "\n",
    "## Inputs\n",
    "* Processed Dataset: `02_PROCESSED_NEA-Seafloor-Litter.csv`. \n",
    "\n",
    "    This served as the initial data source.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* .....\n",
    "\n",
    "\n",
    "## Additional Comments\n",
    "\n",
    "* .....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.1 Research and experiment with the application of data analytics tools, technologies, and methodologies:\n",
    "Explain why you chose specific tools (e.g., comparing different Python libraries like Matplotlib vs. Plotly for visualisation). Detail in Jupyter any experiments with new tools or methodologies, showing code snippets and explaining their results. Include version control commits that show progressive experimentation and refinements of your work. Incorporate new tools/technologies you've researched (such as a new library). The project would show evidence of trial, adaptation, and application in a meaningful way. Focus on challenges encountered and solutions found, explaining how you adapted to using them in the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate proper file access, the working directory is to be adjusted to its parent directory\n",
    "* os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mahahussain/Desktop/NEA-Seafloor-Litter-Analysis/NEA-Seafloor-Litter-Analysis/jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the parent of the current directory the new current directory:\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mahahussain/Desktop/NEA-Seafloor-Litter-Analysis/NEA-Seafloor-Litter-Analysis'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this section:\n",
    "- Introduction to the key statistical concepts used throughout the analysis.\n",
    "- #TODO\n",
    "-\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introduction to Core Statistical Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During data analysis, we want to be able to describe and interpret our dataset. We can do so by utilising statistical concepts that help us identify patterns and draw conclusions.\n",
    "\n",
    "Summary statistics summarise the mean, median, mode, standard deviation and min/max values in one statistic.\n",
    "\n",
    "---\n",
    "\n",
    "1. `MEAN:` The mean is a statistical tool used to find the average of a set of given numbers. \n",
    "\n",
    "- To calculate it, sum all of the values in the dataset and divide that sum by the count. We can use np.mean() to do so.\n",
    "\n",
    "    - Sum: Total amount of the addition of all datapoints in the given numerical column.\n",
    "    - Count: Total number of all datapoints in the set.\n",
    "\n",
    "This is an important foundational principle for data analysis because it provides a measure of central tendency which represents the typical value in a given data column. This allows us to compare averages across different datasets or subgroups within a given dataset. The mean can also be used to examine the change in mean over an extended period of time to view shifts and trends in the data.\n",
    "\n",
    "Note: It is important to note, however, that the mean is sensitive to extreme outliers. In datasets that have highly skewed distribution, the mean may not accurately represent a typical datapoint. Hence, we must investigate the data in conjunctions with other statistical measures.\n",
    "\n",
    "---\n",
    "\n",
    "2. `MEDIAN:` The median is a statistical tool used to find the middle value of an ordered set of given numbers. \n",
    "\n",
    "- To calculate it, you first need to order the datapoints in ascending order.\n",
    "\n",
    "    - If you have an odd number of values, the median is simply the middle number.\n",
    "    - If you have an even number of values, the median is the averaege of the two middle numbers.\n",
    "\n",
    "    We can use np.median() to do so.\n",
    "\n",
    "This is an important foundational principle for data analysis because it helps to identify a typical value of the dataset - 50% of the data is above this point and 50% of the data is below this point.\n",
    "\n",
    "Note: Unlike the mean, the median is not affected by extreme outliers, therefore it is a better measure of central tendency for a dataset with a skewed distribution. If the mean and median values are closely aligned, the data is likely symmetrical with a normal distribution, else, if they are not aligned, the data is likely skewed.\n",
    "\n",
    "---\n",
    "\n",
    "3. `MODE:` The mode is a statistical tool used to find the most frequent value in the given datapoints.\n",
    "\n",
    "- To calculate it, simply count the number of times each value appears in the data.\n",
    "\n",
    "We can use scipy.stats.mode() to do so on a numpy array.\n",
    "\n",
    "This is an important foundational principle for data analysis because because it allows us to identify the most common / popular value in our data. It is especially helpful for analysing categorical types, such as types of litter.\n",
    "    \n",
    "---\n",
    "\n",
    "4. `STANDARD DEVIATION:` Standard deviation is a statistical tool used to measure how spread out the numbers in a dataset are.\n",
    "\n",
    "- To calculate it you can use: np.std(data)\n",
    "- Since we are using DataFrames we can use df.std() from Pandas.\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{\\sum (x_i - \\mu)^2}{N}}\n",
    "$$\n",
    "\n",
    "This is an important foundational principle for data analysis because it tells us how far out the data is from the mean value. \n",
    "\n",
    "- Small standard deviation: The datapoints are close to the average.\n",
    "- Large standard deviation: The datappoints are far from the average.\n",
    "\n",
    "---\n",
    "5. `HYPOTHESIS TESTING:` Hypothesis testing is a statistical concept used to check if our guesses about the data are likely to be true.\n",
    "\n",
    "- It involves setting up a \"null hypothesis\" (a statement we want to test) and then using statistical tests to see if the data provides enough evidence to reject it.\n",
    "\n",
    "This is an important foundational principle for data analysis because it allows us to check if observerd patterns are either likely to be due to chance or if they are statistically significant.\n",
    "\n",
    "---\n",
    "6. `BASIC PROBABILITY:` Probability is a statistical concept used to understand how likely it is that something will happen.\n",
    "\n",
    "- It's expressed as a number between 0 and 1 (or as a percentage). 0 means it's impossible, and 1 (or 100%) means it's certain.\n",
    "\n",
    "This is an important foundational principle for data analysis because it helps us understand the changes of different outcomes. For example, if we want to know the probability of finding a certain type of litter in a specifc area, based on the historical information, we can use probability to understand how likely that is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Statistical Summary for Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides a statistical overview of the numerical data, calculating and interpreting descriptive measures such as mean, median, and standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the processed dataset\n",
    "df = pd.read_csv('data/02_PROCESSED_NEA-Seafloor-Litter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survey</th>\n",
       "      <th>cruise</th>\n",
       "      <th>area</th>\n",
       "      <th>station</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>date</th>\n",
       "      <th>bottle</th>\n",
       "      <th>sheet</th>\n",
       "      <th>bag</th>\n",
       "      <th>...</th>\n",
       "      <th>other.wood</th>\n",
       "      <th>clothing</th>\n",
       "      <th>shoes</th>\n",
       "      <th>other.misc</th>\n",
       "      <th>totallitter</th>\n",
       "      <th>distance</th>\n",
       "      <th>wingspread</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBTS</td>\n",
       "      <td>CIRO 9/92</td>\n",
       "      <td>Greater North Sea</td>\n",
       "      <td>1</td>\n",
       "      <td>51.738333</td>\n",
       "      <td>1.753333</td>\n",
       "      <td>1992-08-14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3794</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IBTS</td>\n",
       "      <td>CIRO 9/92</td>\n",
       "      <td>Greater North Sea</td>\n",
       "      <td>2</td>\n",
       "      <td>51.601667</td>\n",
       "      <td>2.796667</td>\n",
       "      <td>1992-08-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3918</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBTS</td>\n",
       "      <td>CIRO 9/92</td>\n",
       "      <td>Greater North Sea</td>\n",
       "      <td>3</td>\n",
       "      <td>51.823333</td>\n",
       "      <td>3.643333</td>\n",
       "      <td>1992-08-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3624</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IBTS</td>\n",
       "      <td>CIRO 9/92</td>\n",
       "      <td>Greater North Sea</td>\n",
       "      <td>4</td>\n",
       "      <td>52.823333</td>\n",
       "      <td>2.760000</td>\n",
       "      <td>1992-08-16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3642</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IBTS</td>\n",
       "      <td>CIRO 9/92</td>\n",
       "      <td>Greater North Sea</td>\n",
       "      <td>5</td>\n",
       "      <td>52.685000</td>\n",
       "      <td>3.411667</td>\n",
       "      <td>1992-08-16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3791</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  survey     cruise               area  station   Latitude  Longitude  \\\n",
       "0   IBTS  CIRO 9/92  Greater North Sea        1  51.738333   1.753333   \n",
       "1   IBTS  CIRO 9/92  Greater North Sea        2  51.601667   2.796667   \n",
       "2   IBTS  CIRO 9/92  Greater North Sea        3  51.823333   3.643333   \n",
       "3   IBTS  CIRO 9/92  Greater North Sea        4  52.823333   2.760000   \n",
       "4   IBTS  CIRO 9/92  Greater North Sea        5  52.685000   3.411667   \n",
       "\n",
       "         date  bottle  sheet  bag  ...  other.wood  clothing  shoes  \\\n",
       "0  1992-08-14       0      0    1  ...           0         0      0   \n",
       "1  1992-08-15       0      0    1  ...           0         0      0   \n",
       "2  1992-08-15       0      0    0  ...           0         0      0   \n",
       "3  1992-08-16       0      0    1  ...           0         0      0   \n",
       "4  1992-08-16       1      0    1  ...           0         0      0   \n",
       "\n",
       "   other.misc  totallitter  distance  wingspread  year  month  day  \n",
       "0           0            1      3794           0  1992      8   14  \n",
       "1           0            2      3918           0  1992      8   15  \n",
       "2           0            1      3624           0  1992      8   15  \n",
       "3           0            1      3642           0  1992      8   16  \n",
       "4           0            2      3791           0  1992      8   16  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this code is to provide a thorough, non-graphical analysis of the selected columns, offering both general descriptive statistics and more specific measures of central tendency (median and mode). This analysis helps in understanding the distribution and characteristics of the data before proceeding with more advanced statistical tests or visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: totallitter\n",
      "count    4307.000000\n",
      "mean        2.451823\n",
      "std         6.979546\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         1.000000\n",
      "75%         3.000000\n",
      "max       271.000000\n",
      "Name: totallitter, dtype: float64\n",
      "Median: 1.0\n",
      "Mode: 0\n",
      "\n",
      "\n",
      "Column: distance\n",
      "count     4307.000000\n",
      "mean      2071.758068\n",
      "std       2212.877810\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%       2951.000000\n",
      "75%       3696.000000\n",
      "max      13208.000000\n",
      "Name: distance, dtype: float64\n",
      "Median: 2951.0\n",
      "Mode: 0\n",
      "\n",
      "\n",
      "Column: wingspread\n",
      "count    4307.000000\n",
      "mean        3.038078\n",
      "std         4.618863\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         4.000000\n",
      "75%         4.000000\n",
      "max        22.000000\n",
      "Name: wingspread, dtype: float64\n",
      "Median: 4.0\n",
      "Mode: 4\n",
      "\n",
      "\n",
      "Column: year\n",
      "count    4307.000000\n",
      "mean     2007.981658\n",
      "std         7.048329\n",
      "min      1992.000000\n",
      "25%      2005.000000\n",
      "50%      2011.000000\n",
      "75%      2013.000000\n",
      "max      2015.000000\n",
      "Name: year, dtype: float64\n",
      "Median: 2011.0\n",
      "Mode: 2011\n",
      "\n",
      "\n",
      "Column: month\n",
      "count    4307.000000\n",
      "mean        6.879731\n",
      "std         2.710771\n",
      "min         2.000000\n",
      "25%         3.000000\n",
      "50%         8.000000\n",
      "75%         9.000000\n",
      "max        12.000000\n",
      "Name: month, dtype: float64\n",
      "Median: 8.0\n",
      "Mode: 8\n",
      "\n",
      "\n",
      "Column: day\n",
      "count    4307.000000\n",
      "mean       17.098677\n",
      "std         8.281551\n",
      "min         1.000000\n",
      "25%        11.000000\n",
      "50%        18.000000\n",
      "75%        24.000000\n",
      "max        31.000000\n",
      "Name: day, dtype: float64\n",
      "Median: 18.0\n",
      "Modes: 20, 22\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Columns that we want to describe\n",
    "cols_to_describe = ['totallitter', 'distance', 'wingspread', 'year', 'month', 'day']\n",
    "\n",
    "def describe_columns(df, cols):\n",
    "    for col in cols:\n",
    "        print(f\"Column: {col}\")\n",
    "        print(df[col].describe())  # Get the usual describe() output\n",
    "\n",
    "        # Add median\n",
    "        median = df[col].median()\n",
    "        print(f\"Median: {median}\")\n",
    "\n",
    "        # Add mode (handling potential multiple modes)\n",
    "        mode = df[col].mode()\n",
    "        if len(mode) == 1:\n",
    "            print(f\"Mode: {mode[0]}\")\n",
    "        else:\n",
    "            print(f\"Modes: {', '.join(map(str, mode.tolist()))}\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "describe_columns(df, cols_to_describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Insights from Descriptive Statistics (Numerical)\n",
    "\n",
    "### Marine Litter Abundance ('totallitter')\n",
    "- The litter counts show significant variability, with most surveys recording low or no litter.\n",
    "- Extreme accumulation events have been observed, indicating outliers that will require further investigation.\n",
    "\n",
    "### Survey Distance ('distance')\n",
    "- There is substantial variation in the distance covered during surveys.\n",
    "- A significant number of surveys reported minimal or no distance, suggesting a negatively skewed distribution.\n",
    "\n",
    "### Wingspread Measurements ('wingspread')\n",
    "- Most surveys reported a common wingspread measurement, but a large proportion either did not record wingspread data at all.\n",
    "\n",
    "### Temporal Survey Distribution (Year, Month, Day)\n",
    "- Survey activity peaked around 2011, with the majority of surveys conducted in August.\n",
    "- Surveys tended to occur in the latter half of each month, with a slight concentration around the 17th to 22nd days.\n",
    "\n",
    "### General Observations\n",
    "- The high variability across litter counts, survey distances, and wingspread measurements suggests that further investigation into influencing factors is needed.\n",
    "- Skewed distributions in certain variables point to the presence of outliers, which should be handled appropriately in further analyses.\n",
    "- Temporal trends offer valuable insights into the seasonality and long-term patterns of marine litter distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contextualising the AI summary \n",
    "While the descriptive statistics provide a quantitative overview, it is crucial to acknowledge the inherent limitations and potential biases in the data. \n",
    "\n",
    "The observed variability in litter counts and survey distances may reflect not only genuine environmental fluctuations but also variations in survey methodologies, environmental conditions, and observer biases. \n",
    "\n",
    "Furthermore, the temporal distribution of surveys, while seemingly uniform, might be influenced by logistical constraints and funding cycles, which could impact the representativeness of the data.\n",
    "\n",
    "Upon further external research, it was discovered that many surveys may have lacked the necessary sensor to measure the wingspread of the net. \n",
    "\n",
    "According to the International Bottom Trawl Survey Working Group (IBTSWG, 2021), 'Wing spread was not measured by all countries because of missing sensors, and for those countries that did have wingspread sensors, missing values and highly variable observations were common' (IBTSWG, 2021). \n",
    "\n",
    "This issue may have been prevalent in surveys conducted between 1992 and 2014, explaining the absence of wingspread data in a significant portion of the surveys.\n",
    "\n",
    "- [International Bottom Trawl Survey Working Group (IBTSWG), 2021](https://literatur.thuenen.de/digbib_extern/dn064116.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Statistical Summary for Categorical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** To perform a descriptive analysis of categorical variables within the dataset.\n",
    "\n",
    "**Methodology:**\n",
    "Utilise frequency counts to determine the occurrence of each unique value within selected categorical columns.\n",
    "Examine the cardinality of these columns, which refers to the number of unique categories present.\n",
    "\n",
    "**Purpose:**\n",
    "To provide a comprehensive understanding of the distribution and diversity of categorical variables.\n",
    "To contribute to a broader understanding of the dataset's composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Mode of Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode for year: 2011\n",
      "Mode for month: 8\n",
      "Mode for day: 20\n",
      "Mode for survey: IBTS\n",
      "Mode for cruise: CEND 4/15\n",
      "Mode for area: Greater North Sea\n",
      "Mode for station: 0\n"
     ]
    }
   ],
   "source": [
    "cols_for_mode = ['year', 'month', 'day', 'survey', 'cruise', 'area', 'station']\n",
    "\n",
    "def find_modes(df, cols_for_mode):\n",
    "    for col in cols_for_mode:\n",
    "        print(f\"Mode for {col}: {df[col].mode().iloc[0]}\")\n",
    "\n",
    "find_modes(df, cols_for_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Temporal Distribution (Year, Month, Day):**\n",
    "    * The modal year of 2011 suggests a peak in data collection during this period. This concentration may be attributed to specific research initiatives, funding cycles, or environmental events that prompted increased monitoring efforts.\n",
    "\n",
    "    * The modal month of August indicates a seasonal bias in data collection. This could be due to favourable weather conditions, logistical considerations, or the timing of specific research cruises.\n",
    "\n",
    "    * The modal day of the month being the 20th, suggests that data collection tended to happen on this day, however the fact that there are also the 22nd, suggests that data collection happened in the later part of the month.\n",
    "\n",
    "* **Spatial and Survey-Related Distribution (Survey, Cruise, Area, Station):**\n",
    "    * The modal survey 'IBTS' and cruise 'CEND 4/15' highlight the predominant data collection programs contributing to the dataset. \n",
    "\n",
    "    * The modal area 'Greater North Sea' indicates a geographical concentration of data collection. \n",
    "    \n",
    "    * The modal station '0' could indicate a frequently visited location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Using value_counts() to examine frequency of each unique value in select columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical data, representing distinct groups or classifications, requires different statistical approaches compared to numerical data. To analyse such data, frequency counts will be used to assess the unique occurences of each value within select columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Value count for year:\n",
      "year\n",
      "2011    640\n",
      "2010    570\n",
      "2014    500\n",
      "2013    484\n",
      "2012    416\n",
      "2015    277\n",
      "2009    163\n",
      "1994    150\n",
      "1992    149\n",
      "1993    146\n",
      "2008    111\n",
      "2000     92\n",
      "2005     86\n",
      "1999     74\n",
      "1998     74\n",
      "1997     74\n",
      "1995     74\n",
      "1996     72\n",
      "2007     31\n",
      "2003     28\n",
      "2006     27\n",
      "2004     25\n",
      "2001     22\n",
      "2002     22\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for month:\n",
      "month\n",
      "8     1085\n",
      "9      811\n",
      "3      782\n",
      "7      616\n",
      "11     321\n",
      "2      296\n",
      "6      192\n",
      "10     125\n",
      "4       59\n",
      "12      19\n",
      "5        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for day:\n",
      "day\n",
      "20    219\n",
      "22    219\n",
      "17    197\n",
      "18    190\n",
      "19    190\n",
      "21    189\n",
      "28    173\n",
      "25    172\n",
      "23    169\n",
      "26    168\n",
      "16    164\n",
      "27    161\n",
      "24    159\n",
      "13    136\n",
      "15    134\n",
      "1     128\n",
      "12    119\n",
      "14    117\n",
      "2     111\n",
      "30    110\n",
      "9     110\n",
      "29    110\n",
      "3     107\n",
      "8     102\n",
      "7     101\n",
      "5     101\n",
      "6      99\n",
      "10     98\n",
      "4      97\n",
      "11     97\n",
      "31     60\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for survey:\n",
      "survey\n",
      "IBTS                 1575\n",
      "NWGFS                 566\n",
      "Q1SW_with Blinder     496\n",
      "7DBTS                 444\n",
      "Q1SW_No Blinder       408\n",
      "CSEMP                 365\n",
      "Q4SW                  220\n",
      "Q1SW                  138\n",
      "MEMFISH                95\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for cruise:\n",
      "cruise\n",
      "CEND 4/15     277\n",
      "Cend04/14     192\n",
      "Cend5/11      175\n",
      "Cend6/10      172\n",
      "CEND 02/13    142\n",
      "Cend 18/13    118\n",
      "CEND 13/12    118\n",
      "CEND 15/11    116\n",
      "Cend 18/14    112\n",
      "CEND 14/10    112\n",
      "CEND 15/12    108\n",
      "CEND4/11       95\n",
      "Cend 12/10     94\n",
      "CEND 11/12     93\n",
      "CEND 15/14     91\n",
      "CEND 13/10     90\n",
      "CEND 12/09     90\n",
      "Cend 15/13     89\n",
      "CEND 14/11     88\n",
      "CEND 12/13     85\n",
      "CEND5/12       84\n",
      "CEND 13/11     81\n",
      "Cend 17/14     81\n",
      "CEND 17/10     79\n",
      "CIRO 6/00      75\n",
      "CIRO 11/94     75\n",
      "CIRO 9/94      75\n",
      "CIRO 9/92      75\n",
      "CIRO 7/95      74\n",
      "CIRO 7/97      74\n",
      "CIRO 8/93      74\n",
      "CIRO 6/99      74\n",
      "CIRO 11/92     74\n",
      "CEND 15/08     74\n",
      "CIRO 4/98      74\n",
      "Cend 17/09     73\n",
      "CIRO 10/93     72\n",
      "CIRO 8/96      72\n",
      "CEND 19/11     68\n",
      "CEND 13/05     57\n",
      "Cend11/13      50\n",
      "CSEMP2008      37\n",
      "CSEMP2007      31\n",
      "CSEMP2005      29\n",
      "CSEMP2003      28\n",
      "CSEMP2006      27\n",
      "CSEMP2004      25\n",
      "Cend14/14      24\n",
      "CESEMP2010     23\n",
      "CSEMP2002      22\n",
      "CSEMP2001      22\n",
      "CSEMP2000      17\n",
      "CSEMP2011      17\n",
      "CEND9/12       13\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for area:\n",
      "area\n",
      "Greater North Sea                 2241\n",
      "Celtic Seas                       1913\n",
      "Celtic Seas, Greater North Sea     143\n",
      "Unknown                             10\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Value count for station:\n",
      "station\n",
      "0      1031\n",
      "7        39\n",
      "22       38\n",
      "47       37\n",
      "53       37\n",
      "       ... \n",
      "142       1\n",
      "288       1\n",
      "108       1\n",
      "290       1\n",
      "287       1\n",
      "Name: count, Length: 320, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def get_value_counts(data_frame):\n",
    "   \"\"\"\n",
    "    This function shows the frequency of each category for the following columns:\n",
    "    1. year, month, day\n",
    "    2. survey\n",
    "    3. cruise\n",
    "    4. area\n",
    "    5. station\n",
    "\n",
    "   It provides the count of each unique category for the specified columns.\n",
    "\n",
    "    Args: data_frame: The data frame to inspect.\n",
    "    \"\"\"\n",
    "   \n",
    "   # Create a list of the columns to inspect\n",
    "   columns_to_check = ['year', 'month', 'day', 'survey', 'cruise', 'area', 'station']\n",
    "\n",
    "   # For every col in the list, print its title and its value counts\n",
    "   for col in columns_to_check:\n",
    "      print(f\"\\n Value count for {col}:\")\n",
    "      print(data_frame[col].value_counts())\n",
    "    \n",
    "get_value_counts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AI Insights for Value Counts**\n",
    "\n",
    "The frequency distributions of the categorical variables reveal several key patterns within the dataset.\n",
    "\n",
    "**Temporal Distribution (Year, Month, Day):**\n",
    "\n",
    "* **Year:** The data exhibits a concentration of observations in the years 2011, 2010, and 2014, suggesting periods of intensified data collection. This could reflect specific research initiatives or funding cycles. The spread of years from 1992 to 2014 indicates a long-term study.\n",
    "\n",
    "* **Month:** August (8) and September (9) show the highest frequency of observations, indicating a potential seasonal bias in data collection. This may be related to favourable weather conditions or the timing of research cruises.\n",
    "\n",
    "* **Day:** The 20th and 22nd days of the month have the highest counts, suggesting a tendency for surveys to be conducted towards the latter part of the month.\n",
    "\n",
    "**Survey and Spatial Distribution (Survey, Cruise, Area, Station):**\n",
    "\n",
    "* **Survey:** The 'IBTS' survey represents the most frequent data collection effort, indicating its significant contribution to the dataset. Other surveys, such as 'NWGFS' and 'Q1SW', also contribute substantially.\n",
    "\n",
    "* **Cruise:** The 'CEND 4/15' cruise shows the highest frequency, suggesting a focused period of data collection. The variety of cruise names indicates multiple data collection expeditions.\n",
    "\n",
    "* **Area:** The 'Greater North Sea' and 'Celtic Seas' are the predominant areas of data collection, reflecting the geographical focus of the study. The low count for the \"unknown\" category is good, and shows that most locations were recorded.\n",
    "\n",
    "* **Station:** The high frequency of 'station 0' suggests a commonly sampled location or a default designation. The wide range of station values indicates diverse sampling sites.\n",
    "\n",
    "**General Observations:**\n",
    "\n",
    "The value count analysis highlights the variability and distribution of categorical data within the dataset. These findings can inform further analyses, such as exploring potential biases in data collection, investigating temporal and spatial patterns, and understanding the scope of the research efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Using nunique() to Check Cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the cardinality of selected columns, one can quickly assess the variety of categories present and ensure that the dataset's structure aligns with expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique years: 24\n",
      "Number of unique months: 11\n",
      "Number of unique days: 31\n",
      "Number of unique surveys: 9\n",
      "Number of unique cruises: 54\n",
      "Number of unique areas: 4\n",
      "Number of unique stations: 320\n"
     ]
    }
   ],
   "source": [
    "def get_cardinality(data_frame):\n",
    "   \"\"\"\n",
    "    This function shows the count of distinct categories for the following columns:\n",
    "    1. year\n",
    "    2. survey\n",
    "    3. cruise\n",
    "    4. area\n",
    "    5. station\n",
    "\n",
    "    Args: data_frame: The data frame to inspect.\n",
    "    \"\"\"\n",
    "       \n",
    "   # Create a list of the columns to inspect\n",
    "   cols = ['year', 'month', 'day', 'survey', 'cruise', 'area', 'station']\n",
    "\n",
    "   # For every col in the list, print its title and its value counts\n",
    "   for col in cols:\n",
    "      unqiue_count = data_frame[col].nunique()\n",
    "      print(f\"Number of unique {col}s: {unqiue_count}\") \n",
    "  \n",
    "    \n",
    "get_cardinality(df)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique areas: ['Greater North Sea' 'Celtic Seas' 'Celtic Seas, Greater North Sea'\n",
      " 'Unknown']\n",
      "\n",
      "Unique surveys: ['IBTS' 'CSEMP' 'Q4SW' '7DBTS' 'NWGFS' 'Q1SW_with Blinder'\n",
      " 'Q1SW_No Blinder' 'MEMFISH' 'Q1SW']\n"
     ]
    }
   ],
   "source": [
    "# list of unique areas\n",
    "unique_areas = df['area'].unique()\n",
    "print(f\"\\nUnique areas: {unique_areas}\")\n",
    "\n",
    "# list of unique surveys\n",
    "unique_surveys = df['survey'].unique()\n",
    "print(f\"\\nUnique surveys: {unique_surveys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AI Insights for Unique Category Counts**\n",
    "**Year Distribution**\n",
    "- The data spans across **24 unique years**, indicating a broad temporal range of data collection.\n",
    "- The presence of multiple years reflects long-term monitoring, with a focus on both recent and older data points.\n",
    "\n",
    "**Survey Distribution**\n",
    "- **9 unique surveys** are represented, showcasing a diverse range of research efforts.\n",
    "- The presence of multiple surveys suggests a variety of research methodologies and objectives, capturing different aspects of the dataset.\n",
    "\n",
    "**Cruise Distribution**\n",
    "- The data covers **54 unique cruises**, indicating a diverse set of research trips or expeditions.\n",
    "- The variety in cruises suggests both extensive and more specific sampling events across various periods.\n",
    "\n",
    "**Area Distribution**\n",
    "- The data is focused on **2 unique areas**, likely representing two primary regions of interest.\n",
    "- These areas may have distinct environmental conditions, offering opportunities for comparison and regional analysis.\n",
    "\n",
    "**Station Distribution**\n",
    "- The dataset includes data from **320 unique stations**, reflecting a detailed geographical distribution.\n",
    "- The large number of stations suggests a comprehensive sampling effort, providing rich spatial data for analysis.\n",
    "\n",
    "#### **Observations**\n",
    "- The data spans a significant time period with 24 years of collected data.\n",
    "- The variety in the number of surveys, cruises, and stations shows a comprehensive and diverse data collection effort, capturing both broad trends and specific research events.\n",
    "- The focus on two primary areas highlights regional interest, while the extensive number of stations offers a high level of detail and spatial resolution in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Numerical Statistical Tests :Kurtosis, Correlations & Shapiro-Wilk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUMERICAL ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the numerical values that will be used for further non-graphical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['totallitter', 'distance', 'wingspread']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kurtosis Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measure of kurtosis focuses on the distribution of data around the mean and indicates the degree of a distributions tail in relation to a normal distribution. \n",
    "\n",
    "There are three types of kurtosis:\n",
    "\n",
    "1. Mesokurtic: A distribution with normal kurtosis, typically around a value of 3. It indicates that the distribution has a moderate level of peak and tail extremity, similar to the normal distribution.\n",
    "\n",
    "2. Leptokurtic: A distribution with a kurtosis greater than 3, indicating a higher peak and heavier tails compared to a normal distribution. This suggests that the data has more frequent extreme values (outliers) and a sharper peak.\n",
    "\n",
    "3. Platykurtic: A distribution with a kurtosis less than 3, indicating a flatter peak and lighter tails. It suggests that the data has fewer outliers and is more evenly distributed compared to the normal distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurtosis for totallitter: 640.454501760528\n",
      "Kurtosis for distance: 1.7432838000591127\n",
      "Kurtosis for wingspread: 7.566334310633922\n"
     ]
    }
   ],
   "source": [
    "# Measure of kurtosis for select numerical columns\n",
    "\n",
    "def measure_kurtosis(data_frame):\n",
    "    \"\"\"\n",
    "    This function calculates the kurtosis for all selected columns in the dataframe.\n",
    "\n",
    "    Args: data_frame: The data frame to inspect.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # For every numerical column, calculate the kurtosis\n",
    "    for col in columns_to_check:\n",
    "        kurtosis = data_frame[col].kurtosis()\n",
    "        print(f\"Kurtosis for {col}: {kurtosis}\")\n",
    "\n",
    "measure_kurtosis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `totallitter`'s distribution is leptokurtic. The extremely high number indicates the presence of extreme outliers within the data. This may be due to difference in litter collection across the different surveys.\n",
    "\n",
    "2. `distance`'s distribution is platykurtic. In comparison to `totallitter`, it has a relatively normal distribution - suggesting the presence of less outliers and a more evenly distributed set of datapoints.\n",
    "\n",
    "3. `wingspread`'s distribution is Leptokurtic. Whilst not as extreme as `totallitter`'s, it still suggests a sharper peak around the mean of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation analysis can help identify any relationships between numerical variables and assess the strength of a relationship.\n",
    "\n",
    "Spearman correlation was selected for this analysis due to the presence of outliers in the data. Unlike Pearson correlation, which assumes a linear relationship and normal distribution, Spearman correlation is more robust to outliers and does not require the data to follow a normal distribution. \n",
    "\n",
    "This makes Spearman a more suitable choice for assessing the relationships between total litter, distance, and wingspread without being influenced by extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's correlation for totallitter:\n",
      "             totallitter  distance  wingspread\n",
      "totallitter     1.000000 -0.002326    0.053636\n",
      "distance       -0.002326  1.000000   -0.585522\n",
      "wingspread      0.053636 -0.585522    1.000000 \n",
      "\n",
      "Spearman's correlation for distance:\n",
      "             totallitter  distance  wingspread\n",
      "totallitter     1.000000 -0.002326    0.053636\n",
      "distance       -0.002326  1.000000   -0.585522\n",
      "wingspread      0.053636 -0.585522    1.000000 \n",
      "\n",
      "Spearman's correlation for wingspread:\n",
      "             totallitter  distance  wingspread\n",
      "totallitter     1.000000 -0.002326    0.053636\n",
      "distance       -0.002326  1.000000   -0.585522\n",
      "wingspread      0.053636 -0.585522    1.000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def spearmans_correlation(df, cols):\n",
    "    \"\"\"\n",
    "    This function calculates the Spearman's correlation for all selected columns in the dataframe.\n",
    "\n",
    "    Args: data_frame: The data frame to inspect.\n",
    "          cols: The columns to calculate the correlation for.\n",
    "\n",
    "    Prints: The Spearman's correlation for the selected columns.\n",
    "    \"\"\"\n",
    "\n",
    "    for col in columns_to_check:\n",
    "        correlation = df[cols].corr(method='spearman')\n",
    "        print(f\"Spearman's correlation for {col}:\")\n",
    "        print(correlation, \"\\n\")\n",
    "\n",
    "spearmans_correlation(df, columns_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spearman's correlation analysis reveals weak or no significant monotonic relationships between total litter, distance, and wingspread. Specifically, there is a very weak inverse correlation between total litter and distance (-0.0023) and a slight positive correlation between total litter and wingspread (0.0536). However, a moderate negative correlation is observed between distance and wingspread (-0.5855), suggesting that as one increases, the other tends to decrease. Overall, the relationships between these variables are weak, with the exception of the inverse relationship between distance and wingspread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapiro-Wilk test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the statistical techniques covered in the course, I conducted further research on statistical methods for analyzing numerical data. Based on my findings, I chose to apply the Shapiro-Wilk test to assess the normality of the numerical columns in the dataset. \n",
    "\n",
    "For the statistical technique used, the method was sourced from an external resource: \n",
    "[DataCamp - Univariate Investment Risk and Returns](https://campus.datacamp.com/courses/introduction-to-portfolio-risk-management-in-python/univariate-investment-risk-and-returns?ex=12).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk test for totallitter:\n",
      "Statistic: 0.22558233375296244\n",
      "p-value: 4.4889809534015366e-86. The data is not normally distributed.\n",
      "\n",
      "\n",
      "Shapiro-Wilk test for distance:\n",
      "Statistic: 0.7362339750264268\n",
      "p-value: 9.495772723860437e-64. The data is not normally distributed.\n",
      "\n",
      "\n",
      "Shapiro-Wilk test for wingspread:\n",
      "Statistic: 0.5538591374880412\n",
      "p-value: 3.245587655943993e-74. The data is not normally distributed.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import shapiro\n",
    "\n",
    "def shapiro_wilk_test(data_frame, cols):\n",
    "    for col in columns_to_check:\n",
    "        # Perform the Shapiro-Wilk test\n",
    "        stat, p_value = shapiro(data_frame[col])\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Shapiro-Wilk test for {col}:\")\n",
    "        print(f\"Statistic: {stat}\")\n",
    "\n",
    "        # Compare the p-value to the significance level\n",
    "        if p_value < 0.05:\n",
    "            print(f\"p-value: {p_value}. The data is not normally distributed.\")\n",
    "        else:\n",
    "            print(f\"p-value: {p_value}. The data is normally distributed.\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "shapiro_wilk_test(df, columns_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test suggests that the distributions of totallitter, distance, and wingspread deviate significantly from normality, as indicated by the extremely low p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Categorical Statistical Tests: Chi-Squared, Cramers V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chi-Square Test of Independence was conducted to examine whether there is a significant association between the Area of Seas and Survey Type variables, while Cramér’s V was used to quantify the strength of this relationship. These tests help assess dependencies between categorical variables and determine whether any observed relationships are statistically significant. (DataCamp, n.d.; IBM, n.d.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the categorical values that will be used for further non-graphical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_to_check = ['survey', 'area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATEGORICAL ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-Square Test of Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chi-Squared test is a statistical method used to determine if there is a significant association between two categorical variables. \n",
    "\n",
    "It compares the observed frequencies in each category with the frequencies that would be expected if there were no association between the variables. \n",
    "\n",
    "A Chi-Squared value closer to 0 suggests no significant relationship, while a higher value indicates a stronger relationship. \n",
    "\n",
    "The p-value helps assess the significance: if it's below a predefined threshold (commonly 0.05), it suggests that the association between the variables is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-squared test for 'survey' vs 'area':\n",
      "Chi2: 3762.8838\n",
      "p-value: 0.0000\n",
      "Degrees of freedom: 24\n",
      "\n",
      "\n",
      "Expected Frequencies:\n",
      "[[1.97207337e+02 1.47415835e+01 2.31020200e+02 1.03087996e+00]\n",
      " [1.62118644e+02 1.21186441e+01 1.89915254e+02 8.47457627e-01]\n",
      " [6.99553053e+02 5.22927792e+01 8.19497330e+02 3.65683771e+00]\n",
      " [4.21952635e+01 3.15416763e+00 4.94299977e+01 2.20571163e-01]\n",
      " [2.51394938e+02 1.87921987e+01 2.94498723e+02 1.31413977e+00]\n",
      " [6.12941723e+01 4.58184351e+00 7.18035756e+01 3.20408637e-01]\n",
      " [1.81217553e+02 1.35463199e+01 2.12288832e+02 9.47295101e-01]\n",
      " [2.20303692e+02 1.64680752e+01 2.58076619e+02 1.15161365e+00]\n",
      " [9.77153471e+01 7.30438821e+00 1.14469468e+02 5.10796378e-01]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def chi_squared_test(data_frame):\n",
    "    \"\"\"\n",
    "    Performs a Chi-Square test of independence for each specified categorical column against 'survey'.\n",
    "    \n",
    "    Args:\n",
    "    data_frame (pd.DataFrame): The DataFrame containing the data.\n",
    "    cols (list): List of categorical columns to compare with 'survey'.\n",
    "    \n",
    "    Prints:\n",
    "    - Chi-square statistic\n",
    "    - p-value\n",
    "    - Degrees of freedom\n",
    "    - Expected frequencies for each test\n",
    "    \"\"\"\n",
    "    # Step 1: Create a contingency table\n",
    "    contingency_table = pd.crosstab(data_frame['survey'], data_frame['area'])\n",
    "\n",
    "    # Step 2: Perform the chi-squared test\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    print(f\"Chi-squared test for 'survey' vs 'area':\")\n",
    "    print(f\"Chi2: {chi2:.4f}\")\n",
    "    print(f\"p-value: {p:.4f}\")\n",
    "    print(f\"Degrees of freedom: {dof}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Expected Frequencies:\\n{expected}\\n\")\n",
    "\n",
    "    return chi2, p, dof, expected, contingency_table\n",
    "chi2, p_value, dof, expected, contingency_table = chi_squared_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chi-Squared test shows a strong relationship between survey type and area of seas, with a Chi² value of 3762.88 and a p-value of 0.0000. \n",
    "\n",
    "Since the p-value is very small (less than 0.05), we can confidently say that the two variables are not independent. \n",
    "\n",
    "This means that the type of survey conducted varies depending on the sea area. The expected frequencies help us see how different the actual data is from what we would expect if there was no relationship. \n",
    "\n",
    "However, while this tells us there is a connection, it does not show how strong it is, which is where Cramér’s V can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cramers V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cramér’s V is a statistical test used to measure the strength of the relationship between two categorical variables. \n",
    "\n",
    "It gives a value between 0 and 1, where 0 means no association and 1 means a very strong association. Unlike the Chi-Squared test, which only tells us if a relationship exists, Cramér’s V helps us understand how strong that relationship is.\n",
    "\n",
    "It is especially useful when dealing with large datasets where a Chi-Squared test might show significance just because of the large sample size. \n",
    "\n",
    "By calculating Cramér’s V, we can better interpret whether the connection between survey type and area of seas is weak, moderate, or strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cramer's V: 0.5396501996868714\n"
     ]
    }
   ],
   "source": [
    "def cramers_v(chi2_stats, n, contingency_tb):\n",
    "\n",
    "    # Step 1: Minimum of the number of rows and columns in the contingency table\n",
    "    minimum_dimensions= min(contingency_tb.shape) - 1\n",
    "\n",
    "    # Step 2: Calculate Cramer's V\n",
    "    cramer_v = np.sqrt(chi2_stats / (n * minimum_dimensions))\n",
    "    return cramer_v\n",
    "\n",
    "# calculate Cramer's V\n",
    "contigency_table = pd.crosstab(df['survey'], df['area'])\n",
    "n = contingency_table.sum().sum()\n",
    "\n",
    "cramers_v_val = cramers_v(chi2, n, contingency_table)\n",
    "print(f\"Cramer's V: {cramers_v_val}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cramér's V value of 0.54 indicates a moderate strength of this relationship, suggesting a noticeable connection but not an extremely strong one. Furhter analysis will need to be conducted to assess this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Intro to Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why univariate analysis for distrubiotn\n",
    "why distributions\n",
    "why chose to look at x vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analysis of Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analysis of Categorical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualisation Insights & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 : Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Intro to Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Categorical vs. Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Categorical vs. Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Numerical vs. Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualisation Insights & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 : Insights and Interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Summary of Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Discussion of Implications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
